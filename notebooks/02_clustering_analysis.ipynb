{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Repository Clustering and Analysis\n",
    "\n",
    "This notebook demonstrates ghops' powerful clustering capabilities for analyzing and organizing your repository portfolio. You'll learn how to identify similar repositories, find duplicate code, and get consolidation recommendations.\n",
    "\n",
    "## Table of Contents\n",
    "1. [Loading Repository Metadata](#loading-metadata)\n",
    "2. [Understanding Clustering Algorithms](#clustering-algorithms)\n",
    "3. [Running Cluster Analysis](#running-analysis)\n",
    "4. [Visualizing Clusters](#visualizing)\n",
    "5. [Finding Duplicate Code](#duplicates)\n",
    "6. [Getting Consolidation Recommendations](#consolidation)\n",
    "7. [Interactive Exploration](#interactive)\n",
    "8. [Advanced Analysis](#advanced)\n",
    "9. [Exercises](#exercises)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "import tempfile\n",
    "import os\n",
    "\n",
    "# Set up plotting\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "# Helper functions\n",
    "def run_command(cmd):\n",
    "    \"\"\"Run shell command and return output\"\"\"\n",
    "    result = subprocess.run(cmd, shell=True, capture_output=True, text=True)\n",
    "    return result.stdout, result.stderr, result.returncode\n",
    "\n",
    "def parse_jsonl(output):\n",
    "    \"\"\"Parse JSONL output into list of dicts\"\"\"\n",
    "    results = []\n",
    "    for line in output.strip().split('\\n'):\n",
    "        if line:\n",
    "            try:\n",
    "                results.append(json.loads(line))\n",
    "            except json.JSONDecodeError:\n",
    "                continue\n",
    "    return results\n",
    "\n",
    "print(\"Setup complete! Libraries loaded.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Loading Repository Metadata {#loading-metadata}\n",
    "\n",
    "First, let's load metadata about repositories. We'll create a sample dataset for demonstration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create sample repositories with different characteristics\n",
    "temp_dir = tempfile.mkdtemp(prefix=\"ghops_clustering_\")\n",
    "print(f\"Working directory: {temp_dir}\")\n",
    "\n",
    "# Repository templates\n",
    "repo_templates = [\n",
    "    # Python web apps\n",
    "    {\"name\": \"flask-api\", \"lang\": \"python\", \"files\": [\"app.py\", \"requirements.txt\", \"config.py\"]},\n",
    "    {\"name\": \"django-blog\", \"lang\": \"python\", \"files\": [\"manage.py\", \"settings.py\", \"urls.py\"]},\n",
    "    {\"name\": \"fastapi-service\", \"lang\": \"python\", \"files\": [\"main.py\", \"requirements.txt\", \"models.py\"]},\n",
    "    \n",
    "    # JavaScript projects\n",
    "    {\"name\": \"react-app\", \"lang\": \"javascript\", \"files\": [\"package.json\", \"App.js\", \"index.js\"]},\n",
    "    {\"name\": \"vue-dashboard\", \"lang\": \"javascript\", \"files\": [\"package.json\", \"App.vue\", \"main.js\"]},\n",
    "    {\"name\": \"node-api\", \"lang\": \"javascript\", \"files\": [\"package.json\", \"server.js\", \"routes.js\"]},\n",
    "    \n",
    "    # Data science projects\n",
    "    {\"name\": \"ml-pipeline\", \"lang\": \"python\", \"files\": [\"train.py\", \"model.py\", \"data.csv\"]},\n",
    "    {\"name\": \"data-analysis\", \"lang\": \"python\", \"files\": [\"analysis.ipynb\", \"utils.py\", \"data.csv\"]},\n",
    "    \n",
    "    # Duplicate/similar projects\n",
    "    {\"name\": \"flask-api-v2\", \"lang\": \"python\", \"files\": [\"app.py\", \"requirements.txt\", \"config.py\"]},\n",
    "    {\"name\": \"old-react-app\", \"lang\": \"javascript\", \"files\": [\"package.json\", \"App.js\", \"index.js\"]},\n",
    "]\n",
    "\n",
    "# Create the repositories\n",
    "for repo in repo_templates:\n",
    "    repo_path = Path(temp_dir) / repo[\"name\"]\n",
    "    repo_path.mkdir(parents=True)\n",
    "    \n",
    "    # Initialize git\n",
    "    os.chdir(repo_path)\n",
    "    run_command(\"git init\")\n",
    "    run_command(\"git config user.email 'test@example.com'\")\n",
    "    run_command(\"git config user.name 'Test User'\")\n",
    "    \n",
    "    # Create files\n",
    "    for file in repo[\"files\"]:\n",
    "        (repo_path / file).write_text(f\"# {repo['name']}\\n# File: {file}\\n\")\n",
    "    \n",
    "    # Create README with metadata\n",
    "    readme_content = f\"\"\"# {repo['name']}\n",
    "Language: {repo['lang']}\n",
    "Type: {'web' if 'api' in repo['name'] or 'app' in repo['name'] else 'data' if 'ml' in repo['name'] or 'data' in repo['name'] else 'other'}\n",
    "\"\"\"\n",
    "    (repo_path / \"README.md\").write_text(readme_content)\n",
    "    \n",
    "    # Commit\n",
    "    run_command(\"git add .\")\n",
    "    run_command(f\"git commit -m 'Initial commit for {repo['name']}'\")\n",
    "\n",
    "print(f\"Created {len(repo_templates)} sample repositories\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load repository metadata\n",
    "stdout, _, _ = run_command(f\"ghops list {temp_dir}\")\n",
    "repos = parse_jsonl(stdout)\n",
    "\n",
    "# Convert to DataFrame for easier analysis\n",
    "df_repos = pd.DataFrame(repos)\n",
    "print(f\"Loaded {len(df_repos)} repositories\")\n",
    "print(\"\\nRepository Overview:\")\n",
    "df_repos[['name', 'path']].head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Understanding Clustering Algorithms {#clustering-algorithms}\n",
    "\n",
    "ghops supports multiple clustering algorithms, each suited for different analysis needs:\n",
    "\n",
    "- **similarity**: Content-based clustering using file similarity\n",
    "- **language**: Groups repositories by programming language\n",
    "- **size**: Clusters based on repository size and complexity\n",
    "- **activity**: Groups by commit activity patterns\n",
    "- **dependencies**: Clusters based on shared dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check available clustering algorithms\n",
    "stdout, _, _ = run_command(\"ghops cluster --help\")\n",
    "print(\"Available clustering options:\")\n",
    "print(\"=\" * 50)\n",
    "# Parse help text to show algorithms\n",
    "for line in stdout.split('\\n'):\n",
    "    if '--algorithm' in line or 'similarity' in line.lower():\n",
    "        print(line.strip())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Running Cluster Analysis {#running-analysis}\n",
    "\n",
    "Let's run different clustering algorithms and analyze the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run similarity-based clustering\n",
    "stdout, stderr, code = run_command(f\"ghops cluster analyze {temp_dir} --algorithm similarity\")\n",
    "\n",
    "if code == 0:\n",
    "    clusters = parse_jsonl(stdout)\n",
    "    if clusters:\n",
    "        print(\"Similarity-based Clustering Results:\")\n",
    "        print(\"=\" * 50)\n",
    "        \n",
    "        for cluster in clusters:\n",
    "            if 'cluster_id' in cluster:\n",
    "                print(f\"\\nCluster {cluster['cluster_id']}:\")\n",
    "                print(f\"  Members: {cluster.get('members', [])}\")\n",
    "                print(f\"  Similarity: {cluster.get('similarity', 0):.2f}\")\n",
    "    else:\n",
    "        print(\"No clusters found. Creating mock data for demonstration...\")\n",
    "        # Create mock clustering data for demonstration\n",
    "        clusters = [\n",
    "            {\"cluster_id\": 0, \"members\": [\"flask-api\", \"flask-api-v2\", \"fastapi-service\"], \"similarity\": 0.85},\n",
    "            {\"cluster_id\": 1, \"members\": [\"react-app\", \"old-react-app\", \"vue-dashboard\"], \"similarity\": 0.75},\n",
    "            {\"cluster_id\": 2, \"members\": [\"ml-pipeline\", \"data-analysis\"], \"similarity\": 0.65},\n",
    "        ]\n",
    "else:\n",
    "    print(f\"Note: Clustering command not available. Using mock data for demonstration.\")\n",
    "    # Mock data for demonstration\n",
    "    clusters = [\n",
    "        {\"cluster_id\": 0, \"members\": [\"flask-api\", \"flask-api-v2\", \"fastapi-service\"], \"similarity\": 0.85},\n",
    "        {\"cluster_id\": 1, \"members\": [\"react-app\", \"old-react-app\", \"vue-dashboard\"], \"similarity\": 0.75},\n",
    "        {\"cluster_id\": 2, \"members\": [\"ml-pipeline\", \"data-analysis\"], \"similarity\": 0.65},\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze cluster characteristics\n",
    "cluster_df = pd.DataFrame(clusters)\n",
    "\n",
    "if not cluster_df.empty:\n",
    "    print(\"Cluster Statistics:\")\n",
    "    print(\"=\" * 50)\n",
    "    print(f\"Total clusters: {len(cluster_df)}\")\n",
    "    print(f\"Average cluster size: {cluster_df['members'].apply(len).mean():.1f} repositories\")\n",
    "    print(f\"Average similarity: {cluster_df['similarity'].mean():.2f}\")\n",
    "    \n",
    "    # Create cluster size distribution\n",
    "    cluster_sizes = cluster_df['members'].apply(len)\n",
    "    \n",
    "    plt.figure(figsize=(10, 4))\n",
    "    \n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.bar(range(len(cluster_sizes)), cluster_sizes)\n",
    "    plt.xlabel('Cluster ID')\n",
    "    plt.ylabel('Number of Repositories')\n",
    "    plt.title('Cluster Sizes')\n",
    "    \n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.bar(range(len(cluster_df)), cluster_df['similarity'])\n",
    "    plt.xlabel('Cluster ID')\n",
    "    plt.ylabel('Similarity Score')\n",
    "    plt.title('Cluster Similarity Scores')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Visualizing Clusters {#visualizing}\n",
    "\n",
    "Let's create visualizations to better understand the repository relationships."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a similarity matrix\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "# Get all repository names\n",
    "all_repos = []\n",
    "for cluster in clusters:\n",
    "    all_repos.extend(cluster['members'])\n",
    "all_repos = list(set(all_repos))  # Unique repos\n",
    "\n",
    "# Create similarity matrix\n",
    "n_repos = len(all_repos)\n",
    "similarity_matrix = np.zeros((n_repos, n_repos))\n",
    "\n",
    "# Fill similarity matrix based on clusters\n",
    "for cluster in clusters:\n",
    "    members = cluster['members']\n",
    "    sim_score = cluster['similarity']\n",
    "    \n",
    "    for i, repo1 in enumerate(members):\n",
    "        for j, repo2 in enumerate(members):\n",
    "            if repo1 != repo2:\n",
    "                idx1 = all_repos.index(repo1)\n",
    "                idx2 = all_repos.index(repo2)\n",
    "                similarity_matrix[idx1, idx2] = sim_score\n",
    "                similarity_matrix[idx2, idx1] = sim_score\n",
    "\n",
    "# Set diagonal to 1 (self-similarity)\n",
    "np.fill_diagonal(similarity_matrix, 1.0)\n",
    "\n",
    "# Add some noise for repos not in same cluster\n",
    "for i in range(n_repos):\n",
    "    for j in range(i+1, n_repos):\n",
    "        if similarity_matrix[i, j] == 0:\n",
    "            similarity_matrix[i, j] = random.uniform(0.1, 0.3)\n",
    "            similarity_matrix[j, i] = similarity_matrix[i, j]\n",
    "\n",
    "# Create heatmap\n",
    "plt.figure(figsize=(12, 10))\n",
    "sns.heatmap(similarity_matrix, \n",
    "            xticklabels=all_repos, \n",
    "            yticklabels=all_repos,\n",
    "            cmap='YlOrRd',\n",
    "            annot=True,\n",
    "            fmt='.2f',\n",
    "            cbar_kws={'label': 'Similarity Score'})\n",
    "plt.title('Repository Similarity Matrix')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create network graph visualization\n",
    "import networkx as nx\n",
    "\n",
    "# Create graph from similarity matrix\n",
    "G = nx.Graph()\n",
    "\n",
    "# Add nodes\n",
    "for repo in all_repos:\n",
    "    G.add_node(repo)\n",
    "\n",
    "# Add edges for high similarity (> 0.5)\n",
    "threshold = 0.5\n",
    "for i in range(n_repos):\n",
    "    for j in range(i+1, n_repos):\n",
    "        if similarity_matrix[i, j] > threshold:\n",
    "            G.add_edge(all_repos[i], all_repos[j], \n",
    "                      weight=similarity_matrix[i, j])\n",
    "\n",
    "# Create visualization\n",
    "plt.figure(figsize=(14, 10))\n",
    "\n",
    "# Calculate layout\n",
    "pos = nx.spring_layout(G, k=2, iterations=50)\n",
    "\n",
    "# Draw nodes - color by cluster\n",
    "node_colors = []\n",
    "for repo in all_repos:\n",
    "    for i, cluster in enumerate(clusters):\n",
    "        if repo in cluster['members']:\n",
    "            node_colors.append(i)\n",
    "            break\n",
    "    else:\n",
    "        node_colors.append(-1)\n",
    "\n",
    "nx.draw_networkx_nodes(G, pos, node_color=node_colors, \n",
    "                      node_size=1000, cmap='Set1')\n",
    "\n",
    "# Draw edges with varying thickness based on similarity\n",
    "edges = G.edges()\n",
    "weights = [G[u][v]['weight'] * 3 for u, v in edges]\n",
    "nx.draw_networkx_edges(G, pos, width=weights, alpha=0.5)\n",
    "\n",
    "# Draw labels\n",
    "nx.draw_networkx_labels(G, pos, font_size=8)\n",
    "\n",
    "plt.title('Repository Similarity Network\\n(Edges show similarity > 0.5)')\n",
    "plt.axis('off')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Finding Duplicate Code {#duplicates}\n",
    "\n",
    "ghops can identify repositories with duplicate or very similar code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find duplicates\n",
    "stdout, stderr, code = run_command(f\"ghops cluster duplicates {temp_dir}\")\n",
    "\n",
    "# Parse results or use mock data\n",
    "if code == 0 and stdout:\n",
    "    duplicates = parse_jsonl(stdout)\n",
    "else:\n",
    "    # Mock duplicate detection results\n",
    "    duplicates = [\n",
    "        {\n",
    "            \"repo1\": \"flask-api\",\n",
    "            \"repo2\": \"flask-api-v2\",\n",
    "            \"similarity\": 0.95,\n",
    "            \"common_files\": [\"app.py\", \"requirements.txt\", \"config.py\"],\n",
    "            \"identical_files\": [\"requirements.txt\", \"config.py\"],\n",
    "            \"recommendation\": \"Consider merging or archiving flask-api-v2\"\n",
    "        },\n",
    "        {\n",
    "            \"repo1\": \"react-app\",\n",
    "            \"repo2\": \"old-react-app\",\n",
    "            \"similarity\": 0.88,\n",
    "            \"common_files\": [\"package.json\", \"App.js\", \"index.js\"],\n",
    "            \"identical_files\": [\"index.js\"],\n",
    "            \"recommendation\": \"Archive old-react-app if no longer needed\"\n",
    "        }\n",
    "    ]\n",
    "\n",
    "print(\"Duplicate Detection Results:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "for dup in duplicates:\n",
    "    print(f\"\\nPotential Duplicate Pair:\")\n",
    "    print(f\"  {dup['repo1']} <-> {dup['repo2']}\")\n",
    "    print(f\"  Similarity: {dup['similarity']:.1%}\")\n",
    "    print(f\"  Common files: {', '.join(dup['common_files'])}\")\n",
    "    print(f\"  Identical files: {', '.join(dup['identical_files']) if dup['identical_files'] else 'None'}\")\n",
    "    print(f\"  Recommendation: {dup['recommendation']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize duplicate relationships\n",
    "if duplicates:\n",
    "    dup_df = pd.DataFrame(duplicates)\n",
    "    \n",
    "    plt.figure(figsize=(12, 6))\n",
    "    \n",
    "    # Similarity scores\n",
    "    plt.subplot(1, 2, 1)\n",
    "    pairs = [f\"{d['repo1']}\\nvs\\n{d['repo2']}\" for d in duplicates]\n",
    "    similarities = [d['similarity'] for d in duplicates]\n",
    "    \n",
    "    bars = plt.bar(range(len(pairs)), similarities, color=['red' if s > 0.9 else 'orange' if s > 0.7 else 'yellow' for s in similarities])\n",
    "    plt.xticks(range(len(pairs)), pairs, rotation=0)\n",
    "    plt.ylabel('Similarity Score')\n",
    "    plt.title('Duplicate Repository Pairs')\n",
    "    plt.axhline(y=0.9, color='r', linestyle='--', label='High similarity threshold')\n",
    "    plt.axhline(y=0.7, color='orange', linestyle='--', label='Medium similarity threshold')\n",
    "    plt.legend()\n",
    "    \n",
    "    # File overlap\n",
    "    plt.subplot(1, 2, 2)\n",
    "    common_counts = [len(d['common_files']) for d in duplicates]\n",
    "    identical_counts = [len(d['identical_files']) for d in duplicates]\n",
    "    \n",
    "    x = np.arange(len(pairs))\n",
    "    width = 0.35\n",
    "    \n",
    "    plt.bar(x - width/2, common_counts, width, label='Common Files', color='lightblue')\n",
    "    plt.bar(x + width/2, identical_counts, width, label='Identical Files', color='darkblue')\n",
    "    \n",
    "    plt.xticks(x, pairs, rotation=0)\n",
    "    plt.ylabel('Number of Files')\n",
    "    plt.title('File Overlap Analysis')\n",
    "    plt.legend()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Getting Consolidation Recommendations {#consolidation}\n",
    "\n",
    "Based on the clustering analysis, ghops can provide recommendations for consolidating repositories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get consolidation recommendations\n",
    "stdout, stderr, code = run_command(f\"ghops cluster recommend {temp_dir}\")\n",
    "\n",
    "if code == 0 and stdout:\n",
    "    recommendations = parse_jsonl(stdout)\n",
    "else:\n",
    "    # Mock recommendations\n",
    "    recommendations = [\n",
    "        {\n",
    "            \"type\": \"merge\",\n",
    "            \"repos\": [\"flask-api\", \"flask-api-v2\"],\n",
    "            \"reason\": \"95% code similarity, identical dependencies\",\n",
    "            \"action\": \"Merge flask-api-v2 changes into flask-api, then archive flask-api-v2\",\n",
    "            \"effort\": \"low\",\n",
    "            \"impact\": \"high\"\n",
    "        },\n",
    "        {\n",
    "            \"type\": \"archive\",\n",
    "            \"repos\": [\"old-react-app\"],\n",
    "            \"reason\": \"Duplicate of react-app, last updated 6 months ago\",\n",
    "            \"action\": \"Archive old-react-app repository\",\n",
    "            \"effort\": \"minimal\",\n",
    "            \"impact\": \"medium\"\n",
    "        },\n",
    "        {\n",
    "            \"type\": \"refactor\",\n",
    "            \"repos\": [\"ml-pipeline\", \"data-analysis\"],\n",
    "            \"reason\": \"Share common data processing code\",\n",
    "            \"action\": \"Extract shared utilities into a common library\",\n",
    "            \"effort\": \"medium\",\n",
    "            \"impact\": \"high\"\n",
    "        },\n",
    "        {\n",
    "            \"type\": \"organize\",\n",
    "            \"repos\": [\"react-app\", \"vue-dashboard\", \"node-api\"],\n",
    "            \"reason\": \"Related JavaScript projects\",\n",
    "            \"action\": \"Consider monorepo structure for JavaScript projects\",\n",
    "            \"effort\": \"high\",\n",
    "            \"impact\": \"high\"\n",
    "        }\n",
    "    ]\n",
    "\n",
    "print(\"Consolidation Recommendations:\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "for i, rec in enumerate(recommendations, 1):\n",
    "    print(f\"\\n{i}. {rec['type'].upper()} Recommendation\")\n",
    "    print(f\"   Repositories: {', '.join(rec['repos'])}\")\n",
    "    print(f\"   Reason: {rec['reason']}\")\n",
    "    print(f\"   Action: {rec['action']}\")\n",
    "    print(f\"   Effort: {rec['effort']} | Impact: {rec['impact']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create recommendation impact matrix\n",
    "if recommendations:\n",
    "    rec_df = pd.DataFrame(recommendations)\n",
    "    \n",
    "    # Map effort and impact to numeric values\n",
    "    effort_map = {'minimal': 1, 'low': 2, 'medium': 3, 'high': 4}\n",
    "    impact_map = {'low': 1, 'medium': 2, 'high': 3}\n",
    "    \n",
    "    rec_df['effort_score'] = rec_df['effort'].map(effort_map)\n",
    "    rec_df['impact_score'] = rec_df['impact'].map(impact_map)\n",
    "    \n",
    "    plt.figure(figsize=(10, 8))\n",
    "    \n",
    "    # Create scatter plot\n",
    "    colors = {'merge': 'red', 'archive': 'blue', 'refactor': 'green', 'organize': 'purple'}\n",
    "    \n",
    "    for rec_type in rec_df['type'].unique():\n",
    "        mask = rec_df['type'] == rec_type\n",
    "        plt.scatter(rec_df[mask]['effort_score'], \n",
    "                   rec_df[mask]['impact_score'],\n",
    "                   label=rec_type.capitalize(),\n",
    "                   color=colors.get(rec_type, 'gray'),\n",
    "                   s=200,\n",
    "                   alpha=0.7)\n",
    "    \n",
    "    # Add labels\n",
    "    for idx, row in rec_df.iterrows():\n",
    "        plt.annotate(f\"{', '.join(row['repos'][:2])}\",\n",
    "                    (row['effort_score'], row['impact_score']),\n",
    "                    xytext=(5, 5), textcoords='offset points',\n",
    "                    fontsize=8, alpha=0.7)\n",
    "    \n",
    "    # Add quadrant lines\n",
    "    plt.axhline(y=2, color='gray', linestyle='--', alpha=0.3)\n",
    "    plt.axvline(x=2.5, color='gray', linestyle='--', alpha=0.3)\n",
    "    \n",
    "    # Add quadrant labels\n",
    "    plt.text(1.2, 2.7, 'Quick Wins', fontsize=10, alpha=0.5, weight='bold')\n",
    "    plt.text(3.2, 2.7, 'Major Projects', fontsize=10, alpha=0.5, weight='bold')\n",
    "    plt.text(1.2, 1.2, 'Fill-ins', fontsize=10, alpha=0.5, weight='bold')\n",
    "    plt.text(3.2, 1.2, 'Questionable', fontsize=10, alpha=0.5, weight='bold')\n",
    "    \n",
    "    plt.xlabel('Effort Required')\n",
    "    plt.ylabel('Expected Impact')\n",
    "    plt.title('Consolidation Recommendations - Effort vs Impact Matrix')\n",
    "    plt.xticks([1, 2, 3, 4], ['Minimal', 'Low', 'Medium', 'High'])\n",
    "    plt.yticks([1, 2, 3], ['Low', 'Medium', 'High'])\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Interactive Exploration {#interactive}\n",
    "\n",
    "Let's create an interactive tool to explore repository relationships."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Interactive repository explorer\n",
    "from ipywidgets import interact, widgets\n",
    "import IPython.display as display\n",
    "\n",
    "def explore_repository(repo_name):\n",
    "    \"\"\"Interactive function to explore a repository's relationships\"\"\"\n",
    "    print(f\"\\nRepository: {repo_name}\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # Find cluster membership\n",
    "    for cluster in clusters:\n",
    "        if repo_name in cluster['members']:\n",
    "            print(f\"\\nCluster {cluster['cluster_id']}:\")\n",
    "            print(f\"  Cluster members: {', '.join(cluster['members'])}\")\n",
    "            print(f\"  Cluster similarity: {cluster['similarity']:.2f}\")\n",
    "            break\n",
    "    \n",
    "    # Find duplicates\n",
    "    print(\"\\nDuplicate Analysis:\")\n",
    "    found_duplicate = False\n",
    "    for dup in duplicates:\n",
    "        if repo_name in [dup['repo1'], dup['repo2']]:\n",
    "            other = dup['repo2'] if dup['repo1'] == repo_name else dup['repo1']\n",
    "            print(f\"  Potential duplicate: {other} (similarity: {dup['similarity']:.1%})\")\n",
    "            found_duplicate = True\n",
    "    if not found_duplicate:\n",
    "        print(\"  No duplicates found\")\n",
    "    \n",
    "    # Find recommendations\n",
    "    print(\"\\nRecommendations:\")\n",
    "    found_rec = False\n",
    "    for rec in recommendations:\n",
    "        if repo_name in rec['repos']:\n",
    "            print(f\"  {rec['type'].upper()}: {rec['action']}\")\n",
    "            found_rec = True\n",
    "    if not found_rec:\n",
    "        print(\"  No specific recommendations\")\n",
    "    \n",
    "    # Show similarity scores\n",
    "    if repo_name in all_repos:\n",
    "        idx = all_repos.index(repo_name)\n",
    "        similarities = [(all_repos[i], similarity_matrix[idx, i]) \n",
    "                       for i in range(len(all_repos)) if i != idx]\n",
    "        similarities.sort(key=lambda x: x[1], reverse=True)\n",
    "        \n",
    "        print(\"\\nTop 3 Most Similar Repositories:\")\n",
    "        for similar_repo, score in similarities[:3]:\n",
    "            print(f\"  - {similar_repo}: {score:.2f}\")\n",
    "\n",
    "# Create interactive widget\n",
    "if all_repos:\n",
    "    interact(explore_repository, \n",
    "             repo_name=widgets.Dropdown(\n",
    "                 options=sorted(all_repos),\n",
    "                 description='Repository:',\n",
    "                 style={'description_width': 'initial'}\n",
    "             ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Advanced Analysis {#advanced}\n",
    "\n",
    "Let's perform some advanced clustering analysis combining multiple factors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multi-dimensional clustering analysis\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "# Create feature matrix for repositories\n",
    "np.random.seed(42)  # For reproducibility\n",
    "\n",
    "# Generate mock features for each repository\n",
    "features = []\n",
    "feature_names = ['lines_of_code', 'num_files', 'num_commits', 'num_contributors', \n",
    "                 'days_since_update', 'num_dependencies']\n",
    "\n",
    "for repo in all_repos:\n",
    "    # Generate mock features based on repository type\n",
    "    if 'api' in repo or 'service' in repo:\n",
    "        base_features = [2000, 15, 50, 3, 10, 8]\n",
    "    elif 'app' in repo or 'dashboard' in repo:\n",
    "        base_features = [5000, 30, 100, 5, 5, 12]\n",
    "    elif 'ml' in repo or 'data' in repo:\n",
    "        base_features = [1500, 10, 30, 2, 20, 6]\n",
    "    else:\n",
    "        base_features = [1000, 8, 20, 1, 30, 4]\n",
    "    \n",
    "    # Add noise\n",
    "    noisy_features = [int(f * np.random.uniform(0.8, 1.2)) for f in base_features]\n",
    "    features.append(noisy_features)\n",
    "\n",
    "feature_matrix = np.array(features)\n",
    "\n",
    "# Standardize features\n",
    "scaler = StandardScaler()\n",
    "scaled_features = scaler.fit_transform(feature_matrix)\n",
    "\n",
    "# Perform PCA for visualization\n",
    "pca = PCA(n_components=2)\n",
    "pca_features = pca.fit_transform(scaled_features)\n",
    "\n",
    "# Perform K-means clustering\n",
    "kmeans = KMeans(n_clusters=3, random_state=42)\n",
    "cluster_labels = kmeans.fit_predict(scaled_features)\n",
    "\n",
    "# Visualize results\n",
    "plt.figure(figsize=(14, 6))\n",
    "\n",
    "# PCA visualization\n",
    "plt.subplot(1, 2, 1)\n",
    "scatter = plt.scatter(pca_features[:, 0], pca_features[:, 1], \n",
    "                     c=cluster_labels, cmap='viridis', s=100)\n",
    "for i, repo in enumerate(all_repos):\n",
    "    plt.annotate(repo, (pca_features[i, 0], pca_features[i, 1]),\n",
    "                xytext=(5, 5), textcoords='offset points',\n",
    "                fontsize=8, alpha=0.7)\n",
    "plt.xlabel(f'PC1 ({pca.explained_variance_ratio_[0]:.1%} variance)')\n",
    "plt.ylabel(f'PC2 ({pca.explained_variance_ratio_[1]:.1%} variance)')\n",
    "plt.title('Multi-dimensional Clustering (PCA Projection)')\n",
    "plt.colorbar(scatter, label='Cluster')\n",
    "\n",
    "# Feature importance\n",
    "plt.subplot(1, 2, 2)\n",
    "feature_importance = np.abs(pca.components_[0])\n",
    "sorted_idx = np.argsort(feature_importance)[::-1]\n",
    "plt.barh(range(len(feature_names)), feature_importance[sorted_idx])\n",
    "plt.yticks(range(len(feature_names)), [feature_names[i] for i in sorted_idx])\n",
    "plt.xlabel('Importance')\n",
    "plt.title('Feature Importance for Clustering')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print cluster composition\n",
    "print(\"\\nCluster Composition:\")\n",
    "print(\"=\" * 50)\n",
    "for cluster_id in range(3):\n",
    "    cluster_members = [all_repos[i] for i, label in enumerate(cluster_labels) if label == cluster_id]\n",
    "    print(f\"\\nCluster {cluster_id}: {', '.join(cluster_members)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create detailed cluster profile\n",
    "cluster_profiles = []\n",
    "\n",
    "for cluster_id in range(3):\n",
    "    mask = cluster_labels == cluster_id\n",
    "    cluster_features = feature_matrix[mask]\n",
    "    \n",
    "    profile = {\n",
    "        'cluster_id': cluster_id,\n",
    "        'size': mask.sum(),\n",
    "        'avg_lines': cluster_features[:, 0].mean(),\n",
    "        'avg_files': cluster_features[:, 1].mean(),\n",
    "        'avg_commits': cluster_features[:, 2].mean(),\n",
    "        'avg_contributors': cluster_features[:, 3].mean(),\n",
    "        'avg_days_since_update': cluster_features[:, 4].mean(),\n",
    "        'avg_dependencies': cluster_features[:, 5].mean()\n",
    "    }\n",
    "    cluster_profiles.append(profile)\n",
    "\n",
    "profile_df = pd.DataFrame(cluster_profiles)\n",
    "\n",
    "# Display cluster profiles\n",
    "print(\"Cluster Profiles:\")\n",
    "print(\"=\" * 70)\n",
    "print(profile_df.round(1).to_string(index=False))\n",
    "\n",
    "# Radar chart for cluster comparison\n",
    "from math import pi\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 5), subplot_kw=dict(projection='polar'))\n",
    "\n",
    "# Prepare data for radar chart\n",
    "categories = ['Lines', 'Files', 'Commits', 'Contributors', 'Freshness', 'Dependencies']\n",
    "num_vars = len(categories)\n",
    "\n",
    "# Compute angle for each axis\n",
    "angles = [n / float(num_vars) * 2 * pi for n in range(num_vars)]\n",
    "angles += angles[:1]\n",
    "\n",
    "for idx, (ax, profile) in enumerate(zip(axes, cluster_profiles)):\n",
    "    # Normalize values for radar chart\n",
    "    values = [\n",
    "        profile['avg_lines'] / 5000,\n",
    "        profile['avg_files'] / 30,\n",
    "        profile['avg_commits'] / 100,\n",
    "        profile['avg_contributors'] / 5,\n",
    "        1 - (profile['avg_days_since_update'] / 30),  # Invert for freshness\n",
    "        profile['avg_dependencies'] / 12\n",
    "    ]\n",
    "    values += values[:1]\n",
    "    \n",
    "    ax.plot(angles, values, 'o-', linewidth=2)\n",
    "    ax.fill(angles, values, alpha=0.25)\n",
    "    ax.set_xticks(angles[:-1])\n",
    "    ax.set_xticklabels(categories, size=8)\n",
    "    ax.set_ylim(0, 1)\n",
    "    ax.set_title(f'Cluster {idx}', size=11, y=1.1)\n",
    "    ax.grid(True)\n",
    "\n",
    "plt.suptitle('Cluster Characteristic Profiles', size=14, y=1.05)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Exercises {#exercises}\n",
    "\n",
    "Practice your clustering analysis skills with these exercises:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1: Custom Similarity Metric\n",
    "Create a custom similarity metric that considers both code similarity and metadata."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Implement a custom similarity function\n",
    "def custom_similarity(repo1_features, repo2_features):\n",
    "    \"\"\"\n",
    "    Calculate custom similarity between two repositories.\n",
    "    \n",
    "    Consider:\n",
    "    - File overlap\n",
    "    - Language similarity\n",
    "    - Size similarity\n",
    "    - Update frequency similarity\n",
    "    \"\"\"\n",
    "    # Your implementation here\n",
    "    pass\n",
    "\n",
    "# Test your function\n",
    "# similarity = custom_similarity(repo1_data, repo2_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2: Cluster Quality Metrics\n",
    "Evaluate the quality of clustering using silhouette score and other metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Calculate clustering quality metrics\n",
    "from sklearn.metrics import silhouette_score, calinski_harabasz_score\n",
    "\n",
    "# Your code here to:\n",
    "# 1. Calculate silhouette score\n",
    "# 2. Calculate Calinski-Harabasz index\n",
    "# 3. Determine optimal number of clusters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3: Consolidation Plan\n",
    "Create a detailed consolidation plan based on the analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Create a consolidation plan\n",
    "# Your code here to:\n",
    "# 1. Prioritize recommendations by effort/impact\n",
    "# 2. Create a timeline for implementation\n",
    "# 3. Estimate resource savings\n",
    "# 4. Generate a report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean up temporary directory\n",
    "import shutil\n",
    "if 'temp_dir' in locals() and os.path.exists(temp_dir):\n",
    "    shutil.rmtree(temp_dir)\n",
    "    print(f\"Cleaned up temporary directory: {temp_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "In this notebook, you learned:\n",
    "- How to run clustering analysis on repositories\n",
    "- Different clustering algorithms and their use cases\n",
    "- Visualizing repository relationships with heatmaps and network graphs\n",
    "- Identifying duplicate and similar repositories\n",
    "- Getting actionable consolidation recommendations\n",
    "- Performing multi-dimensional analysis with PCA and K-means\n",
    "- Creating interactive exploration tools\n",
    "\n",
    "## Next Steps\n",
    "\n",
    "- **Notebook 3**: Learn about Workflow Orchestration\n",
    "- **Notebook 4**: Explore Advanced Integrations\n",
    "- **Notebook 5**: Dive into Data Analysis and Visualization\n",
    "\n",
    "## Key Takeaways\n",
    "\n",
    "1. Clustering helps identify patterns in your repository portfolio\n",
    "2. Different algorithms reveal different aspects of similarity\n",
    "3. Visualization is crucial for understanding relationships\n",
    "4. Consolidation can significantly reduce maintenance overhead\n",
    "5. Multi-dimensional analysis provides deeper insights"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}