{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Workflow Orchestration with ghops\n",
    "\n",
    "Learn how to automate repository management tasks using ghops' powerful workflow system. Create complex automation pipelines that can maintain, update, and deploy your entire repository portfolio.\n",
    "\n",
    "## Table of Contents\n",
    "1. [Understanding Workflow Concepts](#concepts)\n",
    "2. [Workflow YAML Syntax](#yaml-syntax)\n",
    "3. [Creating Your First Workflow](#first-workflow)\n",
    "4. [Building a Morning Maintenance Routine](#morning-routine)\n",
    "5. [Implementing a Release Pipeline](#release-pipeline)\n",
    "6. [Conditional Execution](#conditional)\n",
    "7. [Parallel and Sequential Steps](#parallel)\n",
    "8. [Monitoring and Debugging](#monitoring)\n",
    "9. [Advanced Workflow Patterns](#advanced)\n",
    "10. [Exercises](#exercises)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "import json\n",
    "import yaml\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "import tempfile\n",
    "import os\n",
    "import time\n",
    "from datetime import datetime, timedelta\n",
    "from IPython.display import display, HTML, clear_output\n",
    "import networkx as nx\n",
    "\n",
    "# Helper functions\n",
    "def run_command(cmd):\n",
    "    \"\"\"Run shell command and return output\"\"\"\n",
    "    result = subprocess.run(cmd, shell=True, capture_output=True, text=True)\n",
    "    return result.stdout, result.stderr, result.returncode\n",
    "\n",
    "def parse_jsonl(output):\n",
    "    \"\"\"Parse JSONL output into list of dicts\"\"\"\n",
    "    results = []\n",
    "    for line in output.strip().split('\\n'):\n",
    "        if line:\n",
    "            try:\n",
    "                results.append(json.loads(line))\n",
    "            except json.JSONDecodeError:\n",
    "                continue\n",
    "    return results\n",
    "\n",
    "# Create workspace\n",
    "workspace = tempfile.mkdtemp(prefix=\"ghops_workflow_\")\n",
    "workflows_dir = Path(workspace) / \"workflows\"\n",
    "workflows_dir.mkdir(exist_ok=True)\n",
    "\n",
    "print(f\"Workspace: {workspace}\")\n",
    "print(f\"Workflows directory: {workflows_dir}\")\n",
    "print(\"Setup complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Understanding Workflow Concepts {#concepts}\n",
    "\n",
    "ghops workflows are automation blueprints that:\n",
    "- **Orchestrate** multiple repository operations\n",
    "- **Schedule** recurring maintenance tasks\n",
    "- **Automate** complex deployment pipelines\n",
    "- **React** to repository changes and events\n",
    "\n",
    "### Key Components\n",
    "\n",
    "1. **Triggers**: When the workflow runs (schedule, event, manual)\n",
    "2. **Steps**: Individual actions to perform\n",
    "3. **Conditions**: Control flow based on state\n",
    "4. **Variables**: Dynamic values and configuration\n",
    "5. **Outputs**: Results and artifacts from the workflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Workflow concept visualization\n",
    "plt.figure(figsize=(12, 8))\n",
    "\n",
    "# Create a directed graph representing workflow concepts\n",
    "G = nx.DiGraph()\n",
    "\n",
    "# Add nodes\n",
    "nodes = [\n",
    "    (\"Trigger\", {\"type\": \"start\", \"color\": \"lightgreen\"}),\n",
    "    (\"Condition\", {\"type\": \"decision\", \"color\": \"yellow\"}),\n",
    "    (\"Step 1\", {\"type\": \"action\", \"color\": \"lightblue\"}),\n",
    "    (\"Step 2\", {\"type\": \"action\", \"color\": \"lightblue\"}),\n",
    "    (\"Parallel A\", {\"type\": \"action\", \"color\": \"lightcoral\"}),\n",
    "    (\"Parallel B\", {\"type\": \"action\", \"color\": \"lightcoral\"}),\n",
    "    (\"Output\", {\"type\": \"end\", \"color\": \"lightgray\"})\n",
    "]\n",
    "\n",
    "for node, attrs in nodes:\n",
    "    G.add_node(node, **attrs)\n",
    "\n",
    "# Add edges\n",
    "edges = [\n",
    "    (\"Trigger\", \"Condition\"),\n",
    "    (\"Condition\", \"Step 1\"),\n",
    "    (\"Condition\", \"Step 2\"),\n",
    "    (\"Step 1\", \"Parallel A\"),\n",
    "    (\"Step 1\", \"Parallel B\"),\n",
    "    (\"Step 2\", \"Output\"),\n",
    "    (\"Parallel A\", \"Output\"),\n",
    "    (\"Parallel B\", \"Output\")\n",
    "]\n",
    "G.add_edges_from(edges)\n",
    "\n",
    "# Layout and draw\n",
    "pos = nx.spring_layout(G, k=2, iterations=50)\n",
    "node_colors = [G.nodes[node]['color'] for node in G.nodes()]\n",
    "\n",
    "nx.draw(G, pos, with_labels=True, node_color=node_colors, \n",
    "        node_size=2000, font_size=10, font_weight='bold',\n",
    "        arrows=True, arrowsize=20, edge_color='gray',\n",
    "        linewidths=2, edgecolors='black')\n",
    "\n",
    "plt.title(\"Workflow Component Flow\", fontsize=14, fontweight='bold')\n",
    "plt.axis('off')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Workflow Components:\")\n",
    "print(\"â€¢ Trigger: Initiates the workflow\")\n",
    "print(\"â€¢ Condition: Determines execution path\")\n",
    "print(\"â€¢ Steps: Sequential actions\")\n",
    "print(\"â€¢ Parallel: Concurrent execution\")\n",
    "print(\"â€¢ Output: Results and artifacts\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Workflow YAML Syntax {#yaml-syntax}\n",
    "\n",
    "Workflows are defined in YAML format. Let's explore the syntax:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example workflow structure\n",
    "example_workflow = {\n",
    "    \"name\": \"example-workflow\",\n",
    "    \"description\": \"Demonstrates workflow syntax\",\n",
    "    \"version\": \"1.0.0\",\n",
    "    \n",
    "    \"triggers\": [\n",
    "        {\"type\": \"schedule\", \"cron\": \"0 9 * * *\"},  # Daily at 9 AM\n",
    "        {\"type\": \"manual\"}  # Can be triggered manually\n",
    "    ],\n",
    "    \n",
    "    \"variables\": {\n",
    "        \"repo_dir\": \"~/projects\",\n",
    "        \"max_age_days\": 7,\n",
    "        \"notification_email\": \"dev@example.com\"\n",
    "    },\n",
    "    \n",
    "    \"steps\": [\n",
    "        {\n",
    "            \"name\": \"scan-repos\",\n",
    "            \"action\": \"ghops.list\",\n",
    "            \"params\": {\n",
    "                \"path\": \"${repo_dir}\",\n",
    "                \"recursive\": True\n",
    "            }\n",
    "        },\n",
    "        {\n",
    "            \"name\": \"check-status\",\n",
    "            \"action\": \"ghops.status\",\n",
    "            \"foreach\": \"${scan-repos.output}\",\n",
    "            \"params\": {\n",
    "                \"path\": \"${item.path}\"\n",
    "            }\n",
    "        },\n",
    "        {\n",
    "            \"name\": \"filter-dirty\",\n",
    "            \"action\": \"filter\",\n",
    "            \"input\": \"${check-status.output}\",\n",
    "            \"condition\": \"item.status.clean == false\"\n",
    "        },\n",
    "        {\n",
    "            \"name\": \"notify\",\n",
    "            \"action\": \"email\",\n",
    "            \"condition\": \"${filter-dirty.output.length} > 0\",\n",
    "            \"params\": {\n",
    "                \"to\": \"${notification_email}\",\n",
    "                \"subject\": \"Repositories need attention\",\n",
    "                \"body\": \"Found ${filter-dirty.output.length} repositories with uncommitted changes\"\n",
    "            }\n",
    "        }\n",
    "    ],\n",
    "    \n",
    "    \"outputs\": {\n",
    "        \"dirty_repos\": \"${filter-dirty.output}\",\n",
    "        \"total_scanned\": \"${scan-repos.output.length}\"\n",
    "    }\n",
    "}\n",
    "\n",
    "# Save as YAML\n",
    "workflow_file = workflows_dir / \"example-workflow.yaml\"\n",
    "with open(workflow_file, 'w') as f:\n",
    "    yaml.dump(example_workflow, f, default_flow_style=False)\n",
    "\n",
    "print(\"Example Workflow YAML:\")\n",
    "print(\"=\" * 60)\n",
    "print(yaml.dump(example_workflow, default_flow_style=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Creating Your First Workflow {#first-workflow}\n",
    "\n",
    "Let's create a simple workflow that checks repository health:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create sample repositories for testing\n",
    "repos_dir = Path(workspace) / \"repos\"\n",
    "repos_dir.mkdir(exist_ok=True)\n",
    "\n",
    "def create_test_repo(name, has_changes=False, has_remote=False):\n",
    "    \"\"\"Create a test repository with specific characteristics\"\"\"\n",
    "    repo_path = repos_dir / name\n",
    "    repo_path.mkdir(exist_ok=True)\n",
    "    \n",
    "    os.chdir(repo_path)\n",
    "    run_command(\"git init\")\n",
    "    run_command(\"git config user.email 'test@example.com'\")\n",
    "    run_command(\"git config user.name 'Test User'\")\n",
    "    \n",
    "    # Create initial files\n",
    "    (repo_path / \"README.md\").write_text(f\"# {name}\")\n",
    "    run_command(\"git add .\")\n",
    "    run_command(\"git commit -m 'Initial commit'\")\n",
    "    \n",
    "    if has_changes:\n",
    "        (repo_path / \"new_file.txt\").write_text(\"Uncommitted changes\")\n",
    "    \n",
    "    if has_remote:\n",
    "        run_command(f\"git remote add origin https://github.com/test/{name}.git\")\n",
    "    \n",
    "    return repo_path\n",
    "\n",
    "# Create test repositories\n",
    "test_repos = [\n",
    "    create_test_repo(\"healthy-repo\", False, True),\n",
    "    create_test_repo(\"dirty-repo\", True, True),\n",
    "    create_test_repo(\"no-remote-repo\", False, False),\n",
    "    create_test_repo(\"abandoned-repo\", True, False)\n",
    "]\n",
    "\n",
    "print(f\"Created {len(test_repos)} test repositories:\")\n",
    "for repo in test_repos:\n",
    "    print(f\"  - {repo.name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create health check workflow\n",
    "health_check_workflow = {\n",
    "    \"name\": \"repository-health-check\",\n",
    "    \"description\": \"Check health of all repositories\",\n",
    "    \"version\": \"1.0.0\",\n",
    "    \n",
    "    \"steps\": [\n",
    "        {\n",
    "            \"name\": \"discover\",\n",
    "            \"action\": \"ghops.list\",\n",
    "            \"params\": {\n",
    "                \"path\": str(repos_dir)\n",
    "            }\n",
    "        },\n",
    "        {\n",
    "            \"name\": \"analyze\",\n",
    "            \"action\": \"ghops.status\",\n",
    "            \"foreach\": \"${discover.output}\",\n",
    "            \"params\": {\n",
    "                \"path\": \"${item.path}\"\n",
    "            }\n",
    "        },\n",
    "        {\n",
    "            \"name\": \"categorize\",\n",
    "            \"action\": \"custom.categorize_health\",\n",
    "            \"input\": \"${analyze.output}\"\n",
    "        },\n",
    "        {\n",
    "            \"name\": \"report\",\n",
    "            \"action\": \"custom.generate_report\",\n",
    "            \"params\": {\n",
    "                \"categories\": \"${categorize.output}\",\n",
    "                \"format\": \"markdown\"\n",
    "            }\n",
    "        }\n",
    "    ],\n",
    "    \n",
    "    \"outputs\": {\n",
    "        \"health_report\": \"${report.output}\",\n",
    "        \"unhealthy_repos\": \"${categorize.output.unhealthy}\"\n",
    "    }\n",
    "}\n",
    "\n",
    "# Save workflow\n",
    "health_workflow_file = workflows_dir / \"health-check.yaml\"\n",
    "with open(health_workflow_file, 'w') as f:\n",
    "    yaml.dump(health_check_workflow, f, default_flow_style=False)\n",
    "\n",
    "print(\"Health Check Workflow created!\")\n",
    "print(f\"Location: {health_workflow_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulate workflow execution\n",
    "def simulate_workflow_execution(workflow, context={}):\n",
    "    \"\"\"Simulate executing a workflow\"\"\"\n",
    "    print(f\"\\nðŸš€ Executing Workflow: {workflow['name']}\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    results = {}\n",
    "    \n",
    "    for step in workflow['steps']:\n",
    "        print(f\"\\nðŸ“Œ Step: {step['name']}\")\n",
    "        print(f\"   Action: {step['action']}\")\n",
    "        \n",
    "        # Simulate different actions\n",
    "        if step['action'] == 'ghops.list':\n",
    "            # Simulate listing repositories\n",
    "            stdout, _, _ = run_command(f\"ghops list {step['params']['path']}\")\n",
    "            if stdout:\n",
    "                results[step['name']] = {'output': parse_jsonl(stdout)}\n",
    "            else:\n",
    "                # Mock data if command not available\n",
    "                results[step['name']] = {'output': [\n",
    "                    {'name': 'healthy-repo', 'path': str(repos_dir / 'healthy-repo')},\n",
    "                    {'name': 'dirty-repo', 'path': str(repos_dir / 'dirty-repo')},\n",
    "                    {'name': 'no-remote-repo', 'path': str(repos_dir / 'no-remote-repo')},\n",
    "                    {'name': 'abandoned-repo', 'path': str(repos_dir / 'abandoned-repo')}\n",
    "                ]}\n",
    "            print(f\"   Found {len(results[step['name']]['output'])} repositories\")\n",
    "            \n",
    "        elif step['action'] == 'ghops.status':\n",
    "            # Simulate status check\n",
    "            statuses = []\n",
    "            if 'foreach' in step:\n",
    "                # Get items from previous step\n",
    "                prev_step = step['foreach'].replace('${', '').replace('.output}', '')\n",
    "                items = results.get(prev_step, {}).get('output', [])\n",
    "                \n",
    "                for item in items:\n",
    "                    # Mock status based on repo name\n",
    "                    status = {\n",
    "                        'name': item['name'],\n",
    "                        'path': item['path'],\n",
    "                        'status': {\n",
    "                            'clean': 'dirty' not in item['name'] and 'abandoned' not in item['name'],\n",
    "                            'has_upstream': 'no-remote' not in item['name'] and 'abandoned' not in item['name'],\n",
    "                            'branch': 'main'\n",
    "                        }\n",
    "                    }\n",
    "                    statuses.append(status)\n",
    "            \n",
    "            results[step['name']] = {'output': statuses}\n",
    "            print(f\"   Checked status of {len(statuses)} repositories\")\n",
    "            \n",
    "        elif step['action'] == 'custom.categorize_health':\n",
    "            # Categorize repositories by health\n",
    "            prev_step = step['input'].replace('${', '').replace('.output}', '')\n",
    "            repos = results.get(prev_step, {}).get('output', [])\n",
    "            \n",
    "            categories = {\n",
    "                'healthy': [],\n",
    "                'needs_attention': [],\n",
    "                'unhealthy': []\n",
    "            }\n",
    "            \n",
    "            for repo in repos:\n",
    "                if repo['status']['clean'] and repo['status']['has_upstream']:\n",
    "                    categories['healthy'].append(repo['name'])\n",
    "                elif not repo['status']['clean']:\n",
    "                    categories['unhealthy'].append(repo['name'])\n",
    "                else:\n",
    "                    categories['needs_attention'].append(repo['name'])\n",
    "            \n",
    "            results[step['name']] = {'output': categories}\n",
    "            print(f\"   Categorized: {len(categories['healthy'])} healthy, \"\n",
    "                  f\"{len(categories['needs_attention'])} need attention, \"\n",
    "                  f\"{len(categories['unhealthy'])} unhealthy\")\n",
    "            \n",
    "        elif step['action'] == 'custom.generate_report':\n",
    "            # Generate report\n",
    "            categories = results.get('categorize', {}).get('output', {})\n",
    "            \n",
    "            report = \"# Repository Health Report\\n\\n\"\n",
    "            report += f\"Generated: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\\n\\n\"\n",
    "            \n",
    "            report += \"## Summary\\n\"\n",
    "            report += f\"- Healthy: {len(categories.get('healthy', []))}\\n\"\n",
    "            report += f\"- Need Attention: {len(categories.get('needs_attention', []))}\\n\"\n",
    "            report += f\"- Unhealthy: {len(categories.get('unhealthy', []))}\\n\\n\"\n",
    "            \n",
    "            for category, repos in categories.items():\n",
    "                report += f\"## {category.replace('_', ' ').title()}\\n\"\n",
    "                for repo in repos:\n",
    "                    report += f\"- {repo}\\n\"\n",
    "                report += \"\\n\"\n",
    "            \n",
    "            results[step['name']] = {'output': report}\n",
    "            print(f\"   Generated report ({len(report)} characters)\")\n",
    "    \n",
    "    # Process outputs\n",
    "    if 'outputs' in workflow:\n",
    "        print(\"\\nðŸ“Š Workflow Outputs:\")\n",
    "        for key, value in workflow['outputs'].items():\n",
    "            print(f\"   {key}: {value}\")\n",
    "    \n",
    "    print(\"\\nâœ… Workflow completed successfully!\")\n",
    "    return results\n",
    "\n",
    "# Execute the workflow\n",
    "results = simulate_workflow_execution(health_check_workflow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the health report\n",
    "if 'report' in results and 'output' in results['report']:\n",
    "    from IPython.display import Markdown\n",
    "    display(Markdown(results['report']['output']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Building a Morning Maintenance Routine {#morning-routine}\n",
    "\n",
    "Create a comprehensive morning maintenance workflow that runs daily:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "morning_routine = {\n",
    "    \"name\": \"morning-maintenance\",\n",
    "    \"description\": \"Daily morning repository maintenance routine\",\n",
    "    \"version\": \"2.0.0\",\n",
    "    \n",
    "    \"triggers\": [\n",
    "        {\"type\": \"schedule\", \"cron\": \"0 6 * * 1-5\"},  # 6 AM weekdays\n",
    "    ],\n",
    "    \n",
    "    \"variables\": {\n",
    "        \"workspace\": \"~/projects\",\n",
    "        \"backup_dir\": \"~/backups/repos\",\n",
    "        \"slack_webhook\": \"${SLACK_WEBHOOK_URL}\",\n",
    "        \"auto_commit\": False,\n",
    "        \"auto_push\": False\n",
    "    },\n",
    "    \n",
    "    \"steps\": [\n",
    "        # Phase 1: Discovery and Analysis\n",
    "        {\n",
    "            \"name\": \"discover_repos\",\n",
    "            \"action\": \"ghops.list\",\n",
    "            \"params\": {\n",
    "                \"path\": \"${workspace}\",\n",
    "                \"recursive\": True\n",
    "            }\n",
    "        },\n",
    "        \n",
    "        # Phase 2: Update and Sync\n",
    "        {\n",
    "            \"name\": \"pull_updates\",\n",
    "            \"action\": \"git.pull\",\n",
    "            \"foreach\": \"${discover_repos.output}\",\n",
    "            \"condition\": \"item.remote.url != null\",\n",
    "            \"params\": {\n",
    "                \"path\": \"${item.path}\",\n",
    "                \"rebase\": True\n",
    "            },\n",
    "            \"on_error\": \"continue\"  # Don't stop on pull errors\n",
    "        },\n",
    "        \n",
    "        # Phase 3: Check Status\n",
    "        {\n",
    "            \"name\": \"check_status\",\n",
    "            \"action\": \"ghops.status\",\n",
    "            \"foreach\": \"${discover_repos.output}\",\n",
    "            \"params\": {\n",
    "                \"path\": \"${item.path}\"\n",
    "            }\n",
    "        },\n",
    "        \n",
    "        # Phase 4: Clean up\n",
    "        {\n",
    "            \"name\": \"cleanup_branches\",\n",
    "            \"action\": \"git.prune\",\n",
    "            \"foreach\": \"${discover_repos.output}\",\n",
    "            \"params\": {\n",
    "                \"path\": \"${item.path}\",\n",
    "                \"remote\": True,\n",
    "                \"merged\": True\n",
    "            }\n",
    "        },\n",
    "        \n",
    "        # Phase 5: Security Scan\n",
    "        {\n",
    "            \"name\": \"security_scan\",\n",
    "            \"action\": \"security.scan\",\n",
    "            \"foreach\": \"${discover_repos.output}\",\n",
    "            \"params\": {\n",
    "                \"path\": \"${item.path}\",\n",
    "                \"check_secrets\": True,\n",
    "                \"check_vulnerabilities\": True\n",
    "            }\n",
    "        },\n",
    "        \n",
    "        # Phase 6: Backup Critical Repos\n",
    "        {\n",
    "            \"name\": \"backup\",\n",
    "            \"action\": \"backup.create\",\n",
    "            \"foreach\": \"${discover_repos.output}\",\n",
    "            \"condition\": \"item.tags.contains('critical')\",\n",
    "            \"params\": {\n",
    "                \"source\": \"${item.path}\",\n",
    "                \"destination\": \"${backup_dir}/${item.name}\",\n",
    "                \"compress\": True\n",
    "            }\n",
    "        },\n",
    "        \n",
    "        # Phase 7: Generate Reports\n",
    "        {\n",
    "            \"name\": \"generate_summary\",\n",
    "            \"action\": \"report.generate\",\n",
    "            \"params\": {\n",
    "                \"status_data\": \"${check_status.output}\",\n",
    "                \"security_data\": \"${security_scan.output}\",\n",
    "                \"format\": \"html\"\n",
    "            }\n",
    "        },\n",
    "        \n",
    "        # Phase 8: Notifications\n",
    "        {\n",
    "            \"name\": \"notify_issues\",\n",
    "            \"action\": \"slack.send\",\n",
    "            \"condition\": \"${security_scan.output.issues.length} > 0\",\n",
    "            \"params\": {\n",
    "                \"webhook\": \"${slack_webhook}\",\n",
    "                \"message\": \"âš ï¸ Security issues found in ${security_scan.output.issues.length} repositories\",\n",
    "                \"attachments\": \"${generate_summary.output}\"\n",
    "            }\n",
    "        }\n",
    "    ],\n",
    "    \n",
    "    \"outputs\": {\n",
    "        \"total_repos\": \"${discover_repos.output.length}\",\n",
    "        \"repos_with_issues\": \"${security_scan.output.issues}\",\n",
    "        \"summary_report\": \"${generate_summary.output}\"\n",
    "    },\n",
    "    \n",
    "    \"on_failure\": {\n",
    "        \"action\": \"email.send\",\n",
    "        \"params\": {\n",
    "            \"to\": \"admin@example.com\",\n",
    "            \"subject\": \"Morning maintenance workflow failed\",\n",
    "            \"body\": \"Error details: ${error.message}\"\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "# Save workflow\n",
    "morning_workflow_file = workflows_dir / \"morning-maintenance.yaml\"\n",
    "with open(morning_workflow_file, 'w') as f:\n",
    "    yaml.dump(morning_routine, f, default_flow_style=False)\n",
    "\n",
    "print(\"Morning Maintenance Workflow:\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Steps: {len(morning_routine['steps'])}\")\n",
    "for step in morning_routine['steps']:\n",
    "    print(f\"  â€¢ {step['name']}: {step['action']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize morning routine workflow\n",
    "plt.figure(figsize=(14, 10))\n",
    "\n",
    "# Create workflow graph\n",
    "G = nx.DiGraph()\n",
    "\n",
    "# Add nodes for each step\n",
    "phases = [\n",
    "    (\"Start\", \"Discovery\", [\"discover_repos\"]),\n",
    "    (\"Discovery\", \"Update\", [\"pull_updates\"]),\n",
    "    (\"Update\", \"Analysis\", [\"check_status\"]),\n",
    "    (\"Analysis\", \"Cleanup\", [\"cleanup_branches\"]),\n",
    "    (\"Cleanup\", \"Security\", [\"security_scan\"]),\n",
    "    (\"Security\", \"Backup\", [\"backup\"]),\n",
    "    (\"Backup\", \"Report\", [\"generate_summary\"]),\n",
    "    (\"Report\", \"Notify\", [\"notify_issues\"]),\n",
    "    (\"Notify\", \"End\", [])\n",
    "]\n",
    "\n",
    "# Add phase nodes\n",
    "for from_phase, to_phase, steps in phases:\n",
    "    G.add_edge(from_phase, to_phase)\n",
    "\n",
    "# Layout and draw\n",
    "pos = nx.spring_layout(G, k=3, iterations=50)\n",
    "\n",
    "# Draw nodes with different colors for different phases\n",
    "node_colors = ['lightgreen' if node == 'Start' else \n",
    "               'lightcoral' if node == 'End' else \n",
    "               'lightblue' for node in G.nodes()]\n",
    "\n",
    "nx.draw(G, pos, with_labels=True, node_color=node_colors,\n",
    "        node_size=2500, font_size=10, font_weight='bold',\n",
    "        arrows=True, arrowsize=20, edge_color='gray',\n",
    "        linewidths=2, edgecolors='black')\n",
    "\n",
    "# Add step details\n",
    "step_text = \"\\n\".join([f\"{i+1}. {step['name']}\" for i, step in enumerate(morning_routine['steps'])])\n",
    "plt.text(1.2, 0.5, step_text, fontsize=8, \n",
    "         bbox=dict(boxstyle=\"round,pad=0.5\", facecolor=\"lightyellow\"))\n",
    "\n",
    "plt.title(\"Morning Maintenance Workflow Phases\", fontsize=14, fontweight='bold')\n",
    "plt.axis('off')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Implementing a Release Pipeline {#release-pipeline}\n",
    "\n",
    "Create a sophisticated release pipeline workflow:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "release_pipeline = {\n",
    "    \"name\": \"release-pipeline\",\n",
    "    \"description\": \"Automated release pipeline for projects\",\n",
    "    \"version\": \"3.0.0\",\n",
    "    \n",
    "    \"parameters\": {\n",
    "        \"project_path\": {\"type\": \"string\", \"required\": True},\n",
    "        \"version\": {\"type\": \"string\", \"required\": True},\n",
    "        \"release_type\": {\"type\": \"string\", \"enum\": [\"major\", \"minor\", \"patch\"]},\n",
    "        \"dry_run\": {\"type\": \"boolean\", \"default\": False}\n",
    "    },\n",
    "    \n",
    "    \"stages\": [\n",
    "        {\n",
    "            \"name\": \"pre-release\",\n",
    "            \"steps\": [\n",
    "                {\n",
    "                    \"name\": \"validate_version\",\n",
    "                    \"action\": \"version.validate\",\n",
    "                    \"params\": {\n",
    "                        \"version\": \"${version}\",\n",
    "                        \"current\": \"${git.tag.latest}\"\n",
    "                    }\n",
    "                },\n",
    "                {\n",
    "                    \"name\": \"run_tests\",\n",
    "                    \"action\": \"test.run\",\n",
    "                    \"params\": {\n",
    "                        \"path\": \"${project_path}\",\n",
    "                        \"coverage\": True,\n",
    "                        \"min_coverage\": 80\n",
    "                    }\n",
    "                },\n",
    "                {\n",
    "                    \"name\": \"lint_code\",\n",
    "                    \"action\": \"lint.run\",\n",
    "                    \"params\": {\n",
    "                        \"path\": \"${project_path}\",\n",
    "                        \"fix\": True\n",
    "                    }\n",
    "                },\n",
    "                {\n",
    "                    \"name\": \"security_audit\",\n",
    "                    \"action\": \"security.audit\",\n",
    "                    \"params\": {\n",
    "                        \"path\": \"${project_path}\",\n",
    "                        \"fail_on_high\": True\n",
    "                    }\n",
    "                }\n",
    "            ]\n",
    "        },\n",
    "        {\n",
    "            \"name\": \"build\",\n",
    "            \"steps\": [\n",
    "                {\n",
    "                    \"name\": \"update_version\",\n",
    "                    \"action\": \"version.update\",\n",
    "                    \"params\": {\n",
    "                        \"path\": \"${project_path}\",\n",
    "                        \"version\": \"${version}\",\n",
    "                        \"files\": [\"setup.py\", \"package.json\", \"Cargo.toml\"]\n",
    "                    }\n",
    "                },\n",
    "                {\n",
    "                    \"name\": \"build_artifacts\",\n",
    "                    \"action\": \"build.create\",\n",
    "                    \"params\": {\n",
    "                        \"path\": \"${project_path}\",\n",
    "                        \"output\": \"./dist\"\n",
    "                    }\n",
    "                },\n",
    "                {\n",
    "                    \"name\": \"generate_changelog\",\n",
    "                    \"action\": \"changelog.generate\",\n",
    "                    \"params\": {\n",
    "                        \"from\": \"${git.tag.latest}\",\n",
    "                        \"to\": \"HEAD\",\n",
    "                        \"format\": \"markdown\"\n",
    "                    }\n",
    "                }\n",
    "            ]\n",
    "        },\n",
    "        {\n",
    "            \"name\": \"release\",\n",
    "            \"condition\": \"${dry_run} == false\",\n",
    "            \"steps\": [\n",
    "                {\n",
    "                    \"name\": \"create_tag\",\n",
    "                    \"action\": \"git.tag\",\n",
    "                    \"params\": {\n",
    "                        \"name\": \"v${version}\",\n",
    "                        \"message\": \"Release version ${version}\",\n",
    "                        \"sign\": True\n",
    "                    }\n",
    "                },\n",
    "                {\n",
    "                    \"name\": \"push_tag\",\n",
    "                    \"action\": \"git.push\",\n",
    "                    \"params\": {\n",
    "                        \"tags\": True\n",
    "                    }\n",
    "                },\n",
    "                {\n",
    "                    \"name\": \"create_github_release\",\n",
    "                    \"action\": \"github.release\",\n",
    "                    \"params\": {\n",
    "                        \"tag\": \"v${version}\",\n",
    "                        \"name\": \"Release ${version}\",\n",
    "                        \"body\": \"${generate_changelog.output}\",\n",
    "                        \"artifacts\": \"${build_artifacts.output}\",\n",
    "                        \"prerelease\": \"${release_type == 'patch'}\"\n",
    "                    }\n",
    "                }\n",
    "            ]\n",
    "        },\n",
    "        {\n",
    "            \"name\": \"deploy\",\n",
    "            \"condition\": \"${dry_run} == false\",\n",
    "            \"parallel\": True,\n",
    "            \"steps\": [\n",
    "                {\n",
    "                    \"name\": \"publish_pypi\",\n",
    "                    \"action\": \"pypi.publish\",\n",
    "                    \"condition\": \"${project.type} == 'python'\",\n",
    "                    \"params\": {\n",
    "                        \"artifacts\": \"${build_artifacts.output}\"\n",
    "                    }\n",
    "                },\n",
    "                {\n",
    "                    \"name\": \"publish_npm\",\n",
    "                    \"action\": \"npm.publish\",\n",
    "                    \"condition\": \"${project.type} == 'javascript'\",\n",
    "                    \"params\": {\n",
    "                        \"artifacts\": \"${build_artifacts.output}\"\n",
    "                    }\n",
    "                },\n",
    "                {\n",
    "                    \"name\": \"deploy_docs\",\n",
    "                    \"action\": \"docs.deploy\",\n",
    "                    \"params\": {\n",
    "                        \"source\": \"./docs\",\n",
    "                        \"target\": \"gh-pages\"\n",
    "                    }\n",
    "                }\n",
    "            ]\n",
    "        },\n",
    "        {\n",
    "            \"name\": \"post-release\",\n",
    "            \"steps\": [\n",
    "                {\n",
    "                    \"name\": \"announce\",\n",
    "                    \"action\": \"social.post\",\n",
    "                    \"params\": {\n",
    "                        \"platforms\": [\"twitter\", \"linkedin\"],\n",
    "                        \"message\": \"ðŸš€ Released version ${version} of ${project.name}! Check out the changelog: ${create_github_release.url}\"\n",
    "                    }\n",
    "                },\n",
    "                {\n",
    "                    \"name\": \"cleanup\",\n",
    "                    \"action\": \"cleanup.artifacts\",\n",
    "                    \"params\": {\n",
    "                        \"path\": \"./dist\"\n",
    "                    }\n",
    "                }\n",
    "            ]\n",
    "        }\n",
    "    ],\n",
    "    \n",
    "    \"rollback\": {\n",
    "        \"enabled\": True,\n",
    "        \"on_failure\": [\n",
    "            {\n",
    "                \"action\": \"git.reset\",\n",
    "                \"params\": {\"hard\": True, \"to\": \"${git.commit.before}\"}\n",
    "            },\n",
    "            {\n",
    "                \"action\": \"notification.send\",\n",
    "                \"params\": {\n",
    "                    \"message\": \"Release ${version} failed and was rolled back\"\n",
    "                }\n",
    "            }\n",
    "        ]\n",
    "    }\n",
    "}\n",
    "\n",
    "# Save workflow\n",
    "release_workflow_file = workflows_dir / \"release-pipeline.yaml\"\n",
    "with open(release_workflow_file, 'w') as f:\n",
    "    yaml.dump(release_pipeline, f, default_flow_style=False)\n",
    "\n",
    "print(\"Release Pipeline Stages:\")\n",
    "print(\"=\" * 60)\n",
    "for stage in release_pipeline['stages']:\n",
    "    print(f\"\\nðŸ“¦ Stage: {stage['name']}\")\n",
    "    if 'condition' in stage:\n",
    "        print(f\"   Condition: {stage['condition']}\")\n",
    "    if stage.get('parallel'):\n",
    "        print(f\"   Execution: Parallel\")\n",
    "    for step in stage['steps']:\n",
    "        print(f\"   - {step['name']}: {step['action']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Conditional Execution {#conditional}\n",
    "\n",
    "Learn how to use conditions to control workflow execution:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Conditional workflow example\n",
    "conditional_workflow = {\n",
    "    \"name\": \"smart-deployment\",\n",
    "    \"description\": \"Deploy based on conditions\",\n",
    "    \n",
    "    \"variables\": {\n",
    "        \"environment\": \"${ENV:-development}\",\n",
    "        \"auto_deploy\": True,\n",
    "        \"min_test_coverage\": 75\n",
    "    },\n",
    "    \n",
    "    \"steps\": [\n",
    "        {\n",
    "            \"name\": \"check_branch\",\n",
    "            \"action\": \"git.current_branch\",\n",
    "            \"output\": \"current_branch\"\n",
    "        },\n",
    "        {\n",
    "            \"name\": \"run_tests\",\n",
    "            \"action\": \"test.run\",\n",
    "            \"output\": \"test_results\"\n",
    "        },\n",
    "        {\n",
    "            \"name\": \"deploy_staging\",\n",
    "            \"action\": \"deploy.staging\",\n",
    "            \"condition\": \"${current_branch} == 'develop' AND ${test_results.passed} == true\"\n",
    "        },\n",
    "        {\n",
    "            \"name\": \"deploy_production\",\n",
    "            \"action\": \"deploy.production\",\n",
    "            \"condition\": \"${current_branch} == 'main' AND ${test_results.coverage} >= ${min_test_coverage} AND ${auto_deploy} == true\"\n",
    "        },\n",
    "        {\n",
    "            \"name\": \"notify_failure\",\n",
    "            \"action\": \"notify.team\",\n",
    "            \"condition\": \"${test_results.passed} == false OR ${test_results.coverage} < ${min_test_coverage}\",\n",
    "            \"params\": {\n",
    "                \"message\": \"Deployment blocked: Tests ${test_results.passed ? 'passed' : 'failed'}, Coverage: ${test_results.coverage}%\"\n",
    "            }\n",
    "        }\n",
    "    ]\n",
    "}\n",
    "\n",
    "# Demonstrate condition evaluation\n",
    "test_scenarios = [\n",
    "    {\"branch\": \"main\", \"passed\": True, \"coverage\": 85},\n",
    "    {\"branch\": \"main\", \"passed\": True, \"coverage\": 70},\n",
    "    {\"branch\": \"develop\", \"passed\": True, \"coverage\": 60},\n",
    "    {\"branch\": \"feature\", \"passed\": False, \"coverage\": 50}\n",
    "]\n",
    "\n",
    "print(\"Conditional Execution Scenarios:\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"{'Branch':<12} {'Tests':<8} {'Coverage':<10} {'Staging':<10} {'Production':<12} {'Notify':<8}\")\n",
    "print(\"-\" * 70)\n",
    "\n",
    "for scenario in test_scenarios:\n",
    "    # Evaluate conditions\n",
    "    deploy_staging = scenario['branch'] == 'develop' and scenario['passed']\n",
    "    deploy_production = scenario['branch'] == 'main' and scenario['coverage'] >= 75 and scenario['passed']\n",
    "    notify = not scenario['passed'] or scenario['coverage'] < 75\n",
    "    \n",
    "    print(f\"{scenario['branch']:<12} \"\n",
    "          f\"{'âœ“' if scenario['passed'] else 'âœ—':<8} \"\n",
    "          f\"{scenario['coverage']}%{'':<7} \"\n",
    "          f\"{'Deploy' if deploy_staging else '-':<10} \"\n",
    "          f\"{'Deploy' if deploy_production else '-':<12} \"\n",
    "          f\"{'Alert' if notify else '-':<8}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Parallel and Sequential Steps {#parallel}\n",
    "\n",
    "Understanding execution modes for optimal performance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "import random\n",
    "\n",
    "# Simulate task execution\n",
    "async def simulate_task(name, duration):\n",
    "    \"\"\"Simulate a task that takes some time\"\"\"\n",
    "    print(f\"  â–¶ Starting: {name}\")\n",
    "    await asyncio.sleep(duration)\n",
    "    print(f\"  âœ“ Completed: {name} ({duration}s)\")\n",
    "    return f\"{name}_result\"\n",
    "\n",
    "async def run_sequential(tasks):\n",
    "    \"\"\"Run tasks sequentially\"\"\"\n",
    "    print(\"\\nðŸ”„ Sequential Execution:\")\n",
    "    print(\"=\" * 40)\n",
    "    start = time.time()\n",
    "    \n",
    "    results = []\n",
    "    for task_name, duration in tasks:\n",
    "        result = await simulate_task(task_name, duration)\n",
    "        results.append(result)\n",
    "    \n",
    "    elapsed = time.time() - start\n",
    "    print(f\"\\nTotal time: {elapsed:.1f}s\")\n",
    "    return results\n",
    "\n",
    "async def run_parallel(tasks):\n",
    "    \"\"\"Run tasks in parallel\"\"\"\n",
    "    print(\"\\nâš¡ Parallel Execution:\")\n",
    "    print(\"=\" * 40)\n",
    "    start = time.time()\n",
    "    \n",
    "    # Create async tasks\n",
    "    async_tasks = [simulate_task(name, duration) for name, duration in tasks]\n",
    "    results = await asyncio.gather(*async_tasks)\n",
    "    \n",
    "    elapsed = time.time() - start\n",
    "    print(f\"\\nTotal time: {elapsed:.1f}s\")\n",
    "    return results\n",
    "\n",
    "# Define tasks with execution times\n",
    "tasks = [\n",
    "    (\"Lint Code\", 2),\n",
    "    (\"Run Tests\", 3),\n",
    "    (\"Build Docker\", 4),\n",
    "    (\"Security Scan\", 2),\n",
    "    (\"Generate Docs\", 1)\n",
    "]\n",
    "\n",
    "# Run comparison\n",
    "print(\"Execution Mode Comparison\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Note: In Jupyter, we need to handle async differently\n",
    "import nest_asyncio\n",
    "nest_asyncio.apply()\n",
    "\n",
    "# Sequential execution\n",
    "loop = asyncio.get_event_loop()\n",
    "seq_results = loop.run_until_complete(run_sequential(tasks))\n",
    "\n",
    "# Parallel execution\n",
    "par_results = loop.run_until_complete(run_parallel(tasks))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize execution timeline\n",
    "fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(12, 6))\n",
    "\n",
    "# Sequential timeline\n",
    "y_pos = 0\n",
    "current_time = 0\n",
    "colors = plt.cm.Set3(range(len(tasks)))\n",
    "\n",
    "for i, (name, duration) in enumerate(tasks):\n",
    "    ax1.barh(y_pos, duration, left=current_time, height=0.8, \n",
    "            color=colors[i], label=name)\n",
    "    ax1.text(current_time + duration/2, y_pos, name, \n",
    "            ha='center', va='center', fontsize=9)\n",
    "    current_time += duration\n",
    "\n",
    "ax1.set_ylim(-0.5, 0.5)\n",
    "ax1.set_xlim(0, current_time + 1)\n",
    "ax1.set_xlabel('Time (seconds)')\n",
    "ax1.set_title('Sequential Execution Timeline')\n",
    "ax1.set_yticks([])\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Parallel timeline\n",
    "for i, (name, duration) in enumerate(tasks):\n",
    "    ax2.barh(i, duration, height=0.8, color=colors[i], label=name)\n",
    "    ax2.text(duration/2, i, name, ha='center', va='center', fontsize=9)\n",
    "\n",
    "ax2.set_ylim(-0.5, len(tasks) - 0.5)\n",
    "ax2.set_xlim(0, max([d for _, d in tasks]) + 1)\n",
    "ax2.set_xlabel('Time (seconds)')\n",
    "ax2.set_ylabel('Task')\n",
    "ax2.set_title('Parallel Execution Timeline')\n",
    "ax2.set_yticks([])\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Performance comparison\n",
    "seq_time = sum(d for _, d in tasks)\n",
    "par_time = max(d for _, d in tasks)\n",
    "speedup = seq_time / par_time\n",
    "\n",
    "print(f\"\\nPerformance Analysis:\")\n",
    "print(f\"  Sequential time: {seq_time}s\")\n",
    "print(f\"  Parallel time: {par_time}s\")\n",
    "print(f\"  Speedup: {speedup:.1f}x\")\n",
    "print(f\"  Efficiency: {speedup/len(tasks)*100:.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Monitoring and Debugging {#monitoring}\n",
    "\n",
    "Learn how to monitor workflow execution and debug issues:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Workflow execution monitor\n",
    "class WorkflowMonitor:\n",
    "    def __init__(self, workflow_name):\n",
    "        self.workflow_name = workflow_name\n",
    "        self.start_time = None\n",
    "        self.steps = []\n",
    "        self.current_step = None\n",
    "        \n",
    "    def start(self):\n",
    "        self.start_time = datetime.now()\n",
    "        print(f\"ðŸš€ Workflow '{self.workflow_name}' started at {self.start_time.strftime('%H:%M:%S')}\")\n",
    "    \n",
    "    def step_start(self, step_name):\n",
    "        self.current_step = {\n",
    "            'name': step_name,\n",
    "            'start': datetime.now(),\n",
    "            'status': 'running'\n",
    "        }\n",
    "        print(f\"  â–¶ Step '{step_name}' started\")\n",
    "    \n",
    "    def step_complete(self, status='success', error=None):\n",
    "        if self.current_step:\n",
    "            self.current_step['end'] = datetime.now()\n",
    "            self.current_step['duration'] = (self.current_step['end'] - self.current_step['start']).total_seconds()\n",
    "            self.current_step['status'] = status\n",
    "            self.current_step['error'] = error\n",
    "            \n",
    "            icon = 'âœ“' if status == 'success' else 'âœ—'\n",
    "            print(f\"  {icon} Step '{self.current_step['name']}' {status} ({self.current_step['duration']:.1f}s)\")\n",
    "            \n",
    "            if error:\n",
    "                print(f\"    Error: {error}\")\n",
    "            \n",
    "            self.steps.append(self.current_step)\n",
    "            self.current_step = None\n",
    "    \n",
    "    def complete(self):\n",
    "        end_time = datetime.now()\n",
    "        total_duration = (end_time - self.start_time).total_seconds()\n",
    "        \n",
    "        successful = sum(1 for s in self.steps if s['status'] == 'success')\n",
    "        failed = sum(1 for s in self.steps if s['status'] == 'failed')\n",
    "        \n",
    "        print(f\"\\nâœ… Workflow completed in {total_duration:.1f}s\")\n",
    "        print(f\"   Steps: {successful} successful, {failed} failed\")\n",
    "        \n",
    "        return self.generate_report()\n",
    "    \n",
    "    def generate_report(self):\n",
    "        report = {\n",
    "            'workflow': self.workflow_name,\n",
    "            'start_time': self.start_time.isoformat(),\n",
    "            'total_duration': (datetime.now() - self.start_time).total_seconds(),\n",
    "            'steps': self.steps,\n",
    "            'summary': {\n",
    "                'total': len(self.steps),\n",
    "                'successful': sum(1 for s in self.steps if s['status'] == 'success'),\n",
    "                'failed': sum(1 for s in self.steps if s['status'] == 'failed'),\n",
    "                'average_duration': sum(s['duration'] for s in self.steps) / len(self.steps) if self.steps else 0\n",
    "            }\n",
    "        }\n",
    "        return report\n",
    "\n",
    "# Simulate monitored workflow execution\n",
    "monitor = WorkflowMonitor(\"test-workflow\")\n",
    "monitor.start()\n",
    "\n",
    "# Simulate steps\n",
    "steps_to_execute = [\n",
    "    (\"Initialize\", 0.5, \"success\", None),\n",
    "    (\"Validate Input\", 0.3, \"success\", None),\n",
    "    (\"Process Data\", 1.2, \"success\", None),\n",
    "    (\"External API Call\", 2.0, \"failed\", \"Connection timeout\"),\n",
    "    (\"Cleanup\", 0.5, \"success\", None)\n",
    "]\n",
    "\n",
    "for step_name, duration, status, error in steps_to_execute:\n",
    "    monitor.step_start(step_name)\n",
    "    time.sleep(duration)  # Simulate work\n",
    "    monitor.step_complete(status, error)\n",
    "\n",
    "# Complete and get report\n",
    "report = monitor.complete()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize workflow execution report\n",
    "if report:\n",
    "    # Create DataFrame from steps\n",
    "    steps_df = pd.DataFrame(report['steps'])\n",
    "    \n",
    "    fig, axes = plt.subplots(2, 2, figsize=(14, 8))\n",
    "    \n",
    "    # Step duration chart\n",
    "    ax1 = axes[0, 0]\n",
    "    colors = ['green' if s == 'success' else 'red' for s in steps_df['status']]\n",
    "    ax1.bar(range(len(steps_df)), steps_df['duration'], color=colors)\n",
    "    ax1.set_xticks(range(len(steps_df)))\n",
    "    ax1.set_xticklabels(steps_df['name'], rotation=45, ha='right')\n",
    "    ax1.set_ylabel('Duration (seconds)')\n",
    "    ax1.set_title('Step Execution Times')\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Success/Failure pie chart\n",
    "    ax2 = axes[0, 1]\n",
    "    status_counts = steps_df['status'].value_counts()\n",
    "    colors = ['#2ecc71' if s == 'success' else '#e74c3c' for s in status_counts.index]\n",
    "    ax2.pie(status_counts.values, labels=status_counts.index, autopct='%1.0f%%',\n",
    "           colors=colors, startangle=90)\n",
    "    ax2.set_title('Step Status Distribution')\n",
    "    \n",
    "    # Timeline visualization\n",
    "    ax3 = axes[1, 0]\n",
    "    for i, row in steps_df.iterrows():\n",
    "        color = 'green' if row['status'] == 'success' else 'red'\n",
    "        ax3.barh(i, row['duration'], left=i*0.5, color=color, alpha=0.7)\n",
    "        ax3.text(i*0.5 + row['duration']/2, i, row['name'], \n",
    "                ha='center', va='center', fontsize=8)\n",
    "    ax3.set_xlabel('Time (relative)')\n",
    "    ax3.set_ylabel('Step')\n",
    "    ax3.set_title('Execution Timeline')\n",
    "    ax3.set_yticks([])\n",
    "    \n",
    "    # Summary text\n",
    "    ax4 = axes[1, 1]\n",
    "    ax4.axis('off')\n",
    "    summary_text = f\"\"\"\n",
    "    Workflow Summary\n",
    "    ================\n",
    "    Total Steps: {report['summary']['total']}\n",
    "    Successful: {report['summary']['successful']}\n",
    "    Failed: {report['summary']['failed']}\n",
    "    \n",
    "    Total Duration: {report['total_duration']:.1f}s\n",
    "    Avg Step Duration: {report['summary']['average_duration']:.1f}s\n",
    "    \n",
    "    Status: {'âš ï¸ Partial Failure' if report['summary']['failed'] > 0 else 'âœ… Success'}\n",
    "    \"\"\"\n",
    "    ax4.text(0.1, 0.5, summary_text, fontsize=11, family='monospace',\n",
    "            bbox=dict(boxstyle=\"round,pad=0.5\", facecolor=\"lightyellow\"))\n",
    "    \n",
    "    plt.suptitle(f\"Workflow Execution Report: {report['workflow']}\", fontsize=14, fontweight='bold')\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Advanced Workflow Patterns {#advanced}\n",
    "\n",
    "Explore advanced patterns for complex automation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Advanced workflow patterns\n",
    "advanced_patterns = {\n",
    "    \"retry_pattern\": {\n",
    "        \"description\": \"Retry failed operations with exponential backoff\",\n",
    "        \"example\": {\n",
    "            \"name\": \"api_call_with_retry\",\n",
    "            \"action\": \"http.request\",\n",
    "            \"retry\": {\n",
    "                \"max_attempts\": 3,\n",
    "                \"backoff\": \"exponential\",\n",
    "                \"initial_delay\": 1,\n",
    "                \"max_delay\": 30\n",
    "            }\n",
    "        }\n",
    "    },\n",
    "    \n",
    "    \"circuit_breaker\": {\n",
    "        \"description\": \"Prevent cascading failures\",\n",
    "        \"example\": {\n",
    "            \"name\": \"protected_service_call\",\n",
    "            \"action\": \"service.call\",\n",
    "            \"circuit_breaker\": {\n",
    "                \"failure_threshold\": 5,\n",
    "                \"timeout\": 60,\n",
    "                \"half_open_requests\": 3\n",
    "            }\n",
    "        }\n",
    "    },\n",
    "    \n",
    "    \"fan_out_fan_in\": {\n",
    "        \"description\": \"Process items in parallel then aggregate results\",\n",
    "        \"example\": {\n",
    "            \"fan_out\": {\n",
    "                \"name\": \"process_items\",\n",
    "                \"action\": \"process\",\n",
    "                \"foreach\": \"${items}\",\n",
    "                \"parallel\": True,\n",
    "                \"max_concurrent\": 5\n",
    "            },\n",
    "            \"fan_in\": {\n",
    "                \"name\": \"aggregate_results\",\n",
    "                \"action\": \"aggregate\",\n",
    "                \"input\": \"${process_items.results}\"\n",
    "            }\n",
    "        }\n",
    "    },\n",
    "    \n",
    "    \"saga_pattern\": {\n",
    "        \"description\": \"Distributed transaction with compensating actions\",\n",
    "        \"example\": {\n",
    "            \"steps\": [\n",
    "                {\n",
    "                    \"name\": \"reserve_inventory\",\n",
    "                    \"action\": \"inventory.reserve\",\n",
    "                    \"compensate\": \"inventory.release\"\n",
    "                },\n",
    "                {\n",
    "                    \"name\": \"charge_payment\",\n",
    "                    \"action\": \"payment.charge\",\n",
    "                    \"compensate\": \"payment.refund\"\n",
    "                },\n",
    "                {\n",
    "                    \"name\": \"ship_order\",\n",
    "                    \"action\": \"shipping.create\",\n",
    "                    \"compensate\": \"shipping.cancel\"\n",
    "                }\n",
    "            ]\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "print(\"Advanced Workflow Patterns:\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for pattern_name, pattern_info in advanced_patterns.items():\n",
    "    print(f\"\\nðŸ“‹ {pattern_name.replace('_', ' ').title()}\")\n",
    "    print(f\"   {pattern_info['description']}\")\n",
    "    print(f\"\\n   Example:\")\n",
    "    print(\"   \" + yaml.dump(pattern_info['example'], default_flow_style=False).replace('\\n', '\\n   '))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Exercises {#exercises}\n",
    "\n",
    "Practice creating and managing workflows:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1: Create a Backup Workflow\n",
    "Design a workflow that backs up critical repositories daily."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Create a backup workflow that:\n",
    "# 1. Identifies repositories tagged as 'critical'\n",
    "# 2. Creates compressed backups\n",
    "# 3. Uploads to cloud storage\n",
    "# 4. Rotates old backups (keep last 7 days)\n",
    "# 5. Sends notification on completion\n",
    "\n",
    "backup_workflow = {\n",
    "    \"name\": \"daily-backup\",\n",
    "    # Your implementation here\n",
    "}\n",
    "\n",
    "# Test your workflow\n",
    "# simulate_workflow_execution(backup_workflow)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2: Implement Error Handling\n",
    "Add comprehensive error handling to a workflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Enhance this workflow with error handling:\n",
    "# 1. Add retry logic for network operations\n",
    "# 2. Implement rollback on failure\n",
    "# 3. Add alerting for critical failures\n",
    "# 4. Create error recovery steps\n",
    "\n",
    "workflow_with_errors = {\n",
    "    \"name\": \"robust-workflow\",\n",
    "    \"steps\": [\n",
    "        # Add your steps with error handling\n",
    "    ]\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3: Performance Optimization\n",
    "Optimize a workflow for better performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Optimize this workflow:\n",
    "# 1. Identify steps that can run in parallel\n",
    "# 2. Add caching where appropriate\n",
    "# 3. Implement batch processing\n",
    "# 4. Add resource limits\n",
    "\n",
    "# Original (slow) workflow\n",
    "slow_workflow = {\n",
    "    \"name\": \"slow-workflow\",\n",
    "    \"steps\": [\n",
    "        {\"name\": \"scan_repo_1\", \"action\": \"scan\"},\n",
    "        {\"name\": \"scan_repo_2\", \"action\": \"scan\"},\n",
    "        {\"name\": \"scan_repo_3\", \"action\": \"scan\"},\n",
    "        {\"name\": \"analyze_1\", \"action\": \"analyze\"},\n",
    "        {\"name\": \"analyze_2\", \"action\": \"analyze\"},\n",
    "        {\"name\": \"analyze_3\", \"action\": \"analyze\"},\n",
    "        {\"name\": \"report\", \"action\": \"generate_report\"}\n",
    "    ]\n",
    "}\n",
    "\n",
    "# Your optimized version\n",
    "optimized_workflow = {\n",
    "    \"name\": \"optimized-workflow\",\n",
    "    # Your implementation here\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean up workspace\n",
    "import shutil\n",
    "if 'workspace' in locals() and os.path.exists(workspace):\n",
    "    shutil.rmtree(workspace)\n",
    "    print(f\"Cleaned up workspace: {workspace}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "In this notebook, you learned:\n",
    "- Core workflow concepts and components\n",
    "- YAML syntax for workflow definition\n",
    "- Creating maintenance and release pipelines\n",
    "- Conditional execution and control flow\n",
    "- Parallel vs sequential execution strategies\n",
    "- Monitoring and debugging workflows\n",
    "- Advanced patterns for robust automation\n",
    "\n",
    "## Next Steps\n",
    "\n",
    "- **Notebook 4**: Advanced Integrations - Combine workflows with clustering\n",
    "- **Notebook 5**: Data Analysis and Visualization\n",
    "\n",
    "## Key Takeaways\n",
    "\n",
    "1. Workflows automate complex multi-step processes\n",
    "2. Conditional logic enables smart automation\n",
    "3. Parallel execution significantly improves performance\n",
    "4. Proper monitoring is essential for production workflows\n",
    "5. Error handling and recovery make workflows robust\n",
    "6. Advanced patterns solve common automation challenges"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}