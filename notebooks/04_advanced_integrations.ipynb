{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Advanced Integrations with ghops\n",
    "\n",
    "This notebook explores advanced integration patterns, combining ghops' powerful features to create sophisticated automation solutions. Learn how to build custom actions, integrate with CI/CD systems, and extend ghops with plugins.\n",
    "\n",
    "## Table of Contents\n",
    "1. [Combining Clustering with Workflows](#combining)\n",
    "2. [Building Custom Actions](#custom-actions)\n",
    "3. [CI/CD Integration](#cicd)\n",
    "4. [Plugin Development](#plugins)\n",
    "5. [API Integration Patterns](#api-patterns)\n",
    "6. [Event-Driven Automation](#event-driven)\n",
    "7. [Cross-Platform Orchestration](#cross-platform)\n",
    "8. [Real-World Use Cases](#use-cases)\n",
    "9. [Performance Optimization](#performance)\n",
    "10. [Exercises](#exercises)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "import json\n",
    "import yaml\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "import tempfile\n",
    "import os\n",
    "import time\n",
    "from datetime import datetime, timedelta\n",
    "import asyncio\n",
    "import requests\n",
    "from typing import Dict, List, Any, Optional\n",
    "from dataclasses import dataclass, field\n",
    "import networkx as nx\n",
    "\n",
    "# Setup\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "workspace = tempfile.mkdtemp(prefix=\"ghops_advanced_\")\n",
    "print(f\"Workspace: {workspace}\")\n",
    "\n",
    "# Helper functions\n",
    "def run_command(cmd):\n",
    "    result = subprocess.run(cmd, shell=True, capture_output=True, text=True)\n",
    "    return result.stdout, result.stderr, result.returncode\n",
    "\n",
    "def parse_jsonl(output):\n",
    "    results = []\n",
    "    for line in output.strip().split('\\n'):\n",
    "        if line:\n",
    "            try:\n",
    "                results.append(json.loads(line))\n",
    "            except json.JSONDecodeError:\n",
    "                continue\n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Combining Clustering with Workflows {#combining}\n",
    "\n",
    "Learn how to use clustering results to drive intelligent workflow decisions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Intelligent maintenance workflow using clustering\n",
    "cluster_driven_workflow = {\n",
    "    \"name\": \"cluster-driven-maintenance\",\n",
    "    \"description\": \"Use clustering to optimize maintenance operations\",\n",
    "    \"version\": \"1.0.0\",\n",
    "    \n",
    "    \"steps\": [\n",
    "        # Step 1: Analyze repository clusters\n",
    "        {\n",
    "            \"name\": \"analyze_clusters\",\n",
    "            \"action\": \"ghops.cluster.analyze\",\n",
    "            \"params\": {\n",
    "                \"path\": \"~/projects\",\n",
    "                \"algorithm\": \"similarity\",\n",
    "                \"threshold\": 0.8\n",
    "            }\n",
    "        },\n",
    "        \n",
    "        # Step 2: Identify duplicate repositories\n",
    "        {\n",
    "            \"name\": \"find_duplicates\",\n",
    "            \"action\": \"ghops.cluster.duplicates\",\n",
    "            \"input\": \"${analyze_clusters.output}\",\n",
    "            \"params\": {\n",
    "                \"min_similarity\": 0.9\n",
    "            }\n",
    "        },\n",
    "        \n",
    "        # Step 3: Process each cluster differently\n",
    "        {\n",
    "            \"name\": \"process_clusters\",\n",
    "            \"action\": \"foreach_cluster\",\n",
    "            \"input\": \"${analyze_clusters.clusters}\",\n",
    "            \"steps\": [\n",
    "                {\n",
    "                    \"name\": \"determine_strategy\",\n",
    "                    \"action\": \"custom.determine_cluster_strategy\",\n",
    "                    \"params\": {\n",
    "                        \"cluster\": \"${item}\",\n",
    "                        \"rules\": {\n",
    "                            \"high_similarity\": \"consolidate\",\n",
    "                            \"medium_similarity\": \"standardize\",\n",
    "                            \"low_similarity\": \"maintain_separately\"\n",
    "                        }\n",
    "                    }\n",
    "                },\n",
    "                {\n",
    "                    \"name\": \"apply_strategy\",\n",
    "                    \"action\": \"switch\",\n",
    "                    \"on\": \"${determine_strategy.output}\",\n",
    "                    \"cases\": {\n",
    "                        \"consolidate\": {\n",
    "                            \"action\": \"workflow.run\",\n",
    "                            \"params\": {\n",
    "                                \"workflow\": \"consolidation-workflow\",\n",
    "                                \"inputs\": {\"repos\": \"${item.members}\"}\n",
    "                            }\n",
    "                        },\n",
    "                        \"standardize\": {\n",
    "                            \"action\": \"workflow.run\",\n",
    "                            \"params\": {\n",
    "                                \"workflow\": \"standardization-workflow\",\n",
    "                                \"inputs\": {\"repos\": \"${item.members}\"}\n",
    "                            }\n",
    "                        },\n",
    "                        \"maintain_separately\": {\n",
    "                            \"action\": \"workflow.run\",\n",
    "                            \"params\": {\n",
    "                                \"workflow\": \"individual-maintenance\",\n",
    "                                \"inputs\": {\"repos\": \"${item.members}\"}\n",
    "                            }\n",
    "                        }\n",
    "                    }\n",
    "                }\n",
    "            ]\n",
    "        },\n",
    "        \n",
    "        # Step 4: Generate optimization report\n",
    "        {\n",
    "            \"name\": \"generate_report\",\n",
    "            \"action\": \"report.optimization\",\n",
    "            \"params\": {\n",
    "                \"clusters\": \"${analyze_clusters.output}\",\n",
    "                \"duplicates\": \"${find_duplicates.output}\",\n",
    "                \"actions_taken\": \"${process_clusters.output}\"\n",
    "            }\n",
    "        }\n",
    "    ]\n",
    "}\n",
    "\n",
    "print(\"Cluster-Driven Workflow Structure:\")\n",
    "print(json.dumps(cluster_driven_workflow, indent=2)[:1000] + \"...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulate cluster-based decision making\n",
    "@dataclass\n",
    "class Cluster:\n",
    "    id: int\n",
    "    members: List[str]\n",
    "    similarity: float\n",
    "    characteristics: Dict[str, Any] = field(default_factory=dict)\n",
    "\n",
    "class ClusterStrategy:\n",
    "    \"\"\"Determines optimal strategy for repository clusters\"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def analyze(cluster: Cluster) -> Dict[str, Any]:\n",
    "        strategy = {\n",
    "            \"cluster_id\": cluster.id,\n",
    "            \"action\": None,\n",
    "            \"priority\": \"medium\",\n",
    "            \"estimated_savings\": 0\n",
    "        }\n",
    "        \n",
    "        if cluster.similarity > 0.9:\n",
    "            strategy[\"action\"] = \"consolidate\"\n",
    "            strategy[\"priority\"] = \"high\"\n",
    "            strategy[\"estimated_savings\"] = len(cluster.members) * 10  # Hours per month\n",
    "            strategy[\"recommendations\"] = [\n",
    "                \"Merge duplicate repositories\",\n",
    "                \"Create shared library for common code\",\n",
    "                \"Archive redundant repos\"\n",
    "            ]\n",
    "        elif cluster.similarity > 0.7:\n",
    "            strategy[\"action\"] = \"standardize\"\n",
    "            strategy[\"priority\"] = \"medium\"\n",
    "            strategy[\"estimated_savings\"] = len(cluster.members) * 5\n",
    "            strategy[\"recommendations\"] = [\n",
    "                \"Align coding standards\",\n",
    "                \"Standardize dependencies\",\n",
    "                \"Create common CI/CD pipeline\"\n",
    "            ]\n",
    "        else:\n",
    "            strategy[\"action\"] = \"optimize_individually\"\n",
    "            strategy[\"priority\"] = \"low\"\n",
    "            strategy[\"estimated_savings\"] = len(cluster.members) * 2\n",
    "            strategy[\"recommendations\"] = [\n",
    "                \"Maintain separate workflows\",\n",
    "                \"Focus on individual optimization\"\n",
    "            ]\n",
    "        \n",
    "        return strategy\n",
    "\n",
    "# Create sample clusters\n",
    "sample_clusters = [\n",
    "    Cluster(0, [\"api-v1\", \"api-v2\", \"api-legacy\"], 0.95),\n",
    "    Cluster(1, [\"frontend-react\", \"frontend-vue\"], 0.75),\n",
    "    Cluster(2, [\"ml-pipeline\", \"data-processor\"], 0.60),\n",
    "    Cluster(3, [\"docs-site\", \"blog\", \"portfolio\"], 0.85)\n",
    "]\n",
    "\n",
    "# Analyze strategies\n",
    "strategies = [ClusterStrategy.analyze(cluster) for cluster in sample_clusters]\n",
    "\n",
    "# Display results\n",
    "strategy_df = pd.DataFrame(strategies)\n",
    "print(\"Cluster Strategy Analysis:\")\n",
    "print(\"=\" * 70)\n",
    "for idx, row in strategy_df.iterrows():\n",
    "    cluster = sample_clusters[idx]\n",
    "    print(f\"\\nCluster {row['cluster_id']}: {', '.join(cluster.members[:2])}...\")\n",
    "    print(f\"  Similarity: {cluster.similarity:.2f}\")\n",
    "    print(f\"  Strategy: {row['action'].upper()}\")\n",
    "    print(f\"  Priority: {row['priority']}\")\n",
    "    print(f\"  Est. Savings: {row['estimated_savings']} hours/month\")\n",
    "    if 'recommendations' in row:\n",
    "        print(\"  Recommendations:\")\n",
    "        for rec in row['recommendations'][:2]:\n",
    "            print(f\"    â€¢ {rec}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Building Custom Actions {#custom-actions}\n",
    "\n",
    "Create reusable custom actions for ghops workflows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom Action Framework\n",
    "from abc import ABC, abstractmethod\n",
    "from typing import Any, Dict, Optional\n",
    "\n",
    "class CustomAction(ABC):\n",
    "    \"\"\"Base class for custom ghops actions\"\"\"\n",
    "    \n",
    "    def __init__(self, name: str, version: str = \"1.0.0\"):\n",
    "        self.name = name\n",
    "        self.version = version\n",
    "        self.metadata = {\n",
    "            \"created\": datetime.now().isoformat(),\n",
    "            \"author\": \"ghops\"\n",
    "        }\n",
    "    \n",
    "    @abstractmethod\n",
    "    def validate_params(self, params: Dict[str, Any]) -> bool:\n",
    "        \"\"\"Validate action parameters\"\"\"\n",
    "        pass\n",
    "    \n",
    "    @abstractmethod\n",
    "    def execute(self, context: Dict[str, Any], params: Dict[str, Any]) -> Dict[str, Any]:\n",
    "        \"\"\"Execute the action\"\"\"\n",
    "        pass\n",
    "    \n",
    "    def run(self, context: Dict[str, Any], params: Dict[str, Any]) -> Dict[str, Any]:\n",
    "        \"\"\"Run the action with validation\"\"\"\n",
    "        if not self.validate_params(params):\n",
    "            raise ValueError(f\"Invalid parameters for action {self.name}\")\n",
    "        \n",
    "        result = {\n",
    "            \"action\": self.name,\n",
    "            \"version\": self.version,\n",
    "            \"start_time\": datetime.now().isoformat()\n",
    "        }\n",
    "        \n",
    "        try:\n",
    "            output = self.execute(context, params)\n",
    "            result[\"status\"] = \"success\"\n",
    "            result[\"output\"] = output\n",
    "        except Exception as e:\n",
    "            result[\"status\"] = \"failed\"\n",
    "            result[\"error\"] = str(e)\n",
    "        \n",
    "        result[\"end_time\"] = datetime.now().isoformat()\n",
    "        return result\n",
    "\n",
    "# Example Custom Actions\n",
    "class CodeQualityAction(CustomAction):\n",
    "    \"\"\"Analyze code quality across repositories\"\"\"\n",
    "    \n",
    "    def validate_params(self, params: Dict[str, Any]) -> bool:\n",
    "        return \"path\" in params\n",
    "    \n",
    "    def execute(self, context: Dict[str, Any], params: Dict[str, Any]) -> Dict[str, Any]:\n",
    "        path = params[\"path\"]\n",
    "        \n",
    "        # Simulate code quality analysis\n",
    "        metrics = {\n",
    "            \"complexity\": np.random.randint(1, 10),\n",
    "            \"maintainability\": np.random.randint(60, 100),\n",
    "            \"test_coverage\": np.random.randint(40, 95),\n",
    "            \"code_smells\": np.random.randint(0, 20),\n",
    "            \"duplicates\": np.random.randint(0, 10),\n",
    "            \"technical_debt\": np.random.randint(0, 100)\n",
    "        }\n",
    "        \n",
    "        grade = \"A\" if metrics[\"maintainability\"] > 80 else \"B\" if metrics[\"maintainability\"] > 60 else \"C\"\n",
    "        \n",
    "        return {\n",
    "            \"path\": path,\n",
    "            \"metrics\": metrics,\n",
    "            \"grade\": grade,\n",
    "            \"recommendations\": self._generate_recommendations(metrics)\n",
    "        }\n",
    "    \n",
    "    def _generate_recommendations(self, metrics: Dict[str, int]) -> List[str]:\n",
    "        recommendations = []\n",
    "        if metrics[\"complexity\"] > 7:\n",
    "            recommendations.append(\"Refactor complex functions\")\n",
    "        if metrics[\"test_coverage\"] < 70:\n",
    "            recommendations.append(\"Increase test coverage\")\n",
    "        if metrics[\"code_smells\"] > 10:\n",
    "            recommendations.append(\"Address code smells\")\n",
    "        return recommendations\n",
    "\n",
    "class DependencyAuditAction(CustomAction):\n",
    "    \"\"\"Audit dependencies for security and updates\"\"\"\n",
    "    \n",
    "    def validate_params(self, params: Dict[str, Any]) -> bool:\n",
    "        return \"path\" in params\n",
    "    \n",
    "    def execute(self, context: Dict[str, Any], params: Dict[str, Any]) -> Dict[str, Any]:\n",
    "        path = params[\"path\"]\n",
    "        \n",
    "        # Simulate dependency audit\n",
    "        vulnerabilities = [\n",
    "            {\"package\": \"requests\", \"severity\": \"high\", \"version\": \"2.25.0\", \"fix\": \"2.28.0\"},\n",
    "            {\"package\": \"pyyaml\", \"severity\": \"medium\", \"version\": \"5.3\", \"fix\": \"5.4\"}\n",
    "        ] if np.random.random() > 0.5 else []\n",
    "        \n",
    "        outdated = [\n",
    "            {\"package\": \"numpy\", \"current\": \"1.19.0\", \"latest\": \"1.24.0\"},\n",
    "            {\"package\": \"pandas\", \"current\": \"1.2.0\", \"latest\": \"2.0.0\"}\n",
    "        ]\n",
    "        \n",
    "        return {\n",
    "            \"path\": path,\n",
    "            \"vulnerabilities\": vulnerabilities,\n",
    "            \"outdated_packages\": outdated,\n",
    "            \"total_dependencies\": np.random.randint(10, 50),\n",
    "            \"risk_level\": \"high\" if vulnerabilities else \"low\"\n",
    "        }\n",
    "\n",
    "# Register custom actions\n",
    "custom_actions = {\n",
    "    \"code_quality\": CodeQualityAction(\"code_quality\"),\n",
    "    \"dependency_audit\": DependencyAuditAction(\"dependency_audit\")\n",
    "}\n",
    "\n",
    "# Test custom actions\n",
    "print(\"Testing Custom Actions:\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for action_name, action in custom_actions.items():\n",
    "    result = action.run({}, {\"path\": \"/test/repo\"})\n",
    "    print(f\"\\n{action_name}:\")\n",
    "    print(f\"  Status: {result['status']}\")\n",
    "    if 'output' in result:\n",
    "        output = result['output']\n",
    "        if 'grade' in output:\n",
    "            print(f\"  Grade: {output['grade']}\")\n",
    "        if 'risk_level' in output:\n",
    "            print(f\"  Risk Level: {output['risk_level']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. CI/CD Integration {#cicd}\n",
    "\n",
    "Integrate ghops with popular CI/CD systems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GitHub Actions Integration\n",
    "github_action_workflow = \"\"\"\n",
    "name: ghops Repository Management\n",
    "on:\n",
    "  schedule:\n",
    "    - cron: '0 6 * * *'  # Daily at 6 AM\n",
    "  push:\n",
    "    branches: [main]\n",
    "  workflow_dispatch:\n",
    "\n",
    "jobs:\n",
    "  repository-analysis:\n",
    "    runs-on: ubuntu-latest\n",
    "    steps:\n",
    "      - uses: actions/checkout@v3\n",
    "      \n",
    "      - name: Setup Python\n",
    "        uses: actions/setup-python@v4\n",
    "        with:\n",
    "          python-version: '3.9'\n",
    "      \n",
    "      - name: Install ghops\n",
    "        run: pip install ghops\n",
    "      \n",
    "      - name: Run Repository Clustering\n",
    "        run: |\n",
    "          ghops cluster analyze . --algorithm similarity > clusters.json\n",
    "          ghops cluster duplicates . > duplicates.json\n",
    "      \n",
    "      - name: Generate Report\n",
    "        run: |\n",
    "          ghops report generate \\\n",
    "            --clusters clusters.json \\\n",
    "            --duplicates duplicates.json \\\n",
    "            --format markdown > report.md\n",
    "      \n",
    "      - name: Upload Report\n",
    "        uses: actions/upload-artifact@v3\n",
    "        with:\n",
    "          name: repository-report\n",
    "          path: report.md\n",
    "      \n",
    "      - name: Comment on PR\n",
    "        if: github.event_name == 'pull_request'\n",
    "        uses: actions/github-script@v6\n",
    "        with:\n",
    "          script: |\n",
    "            const fs = require('fs');\n",
    "            const report = fs.readFileSync('report.md', 'utf8');\n",
    "            github.rest.issues.createComment({\n",
    "              issue_number: context.issue.number,\n",
    "              owner: context.repo.owner,\n",
    "              repo: context.repo.repo,\n",
    "              body: report\n",
    "            });\n",
    "\"\"\"\n",
    "\n",
    "# GitLab CI Integration\n",
    "gitlab_ci_config = \"\"\"\n",
    "stages:\n",
    "  - analysis\n",
    "  - optimization\n",
    "  - deploy\n",
    "\n",
    "variables:\n",
    "  GHOPS_CONFIG: /etc/ghops/config.yaml\n",
    "\n",
    "repository-analysis:\n",
    "  stage: analysis\n",
    "  image: python:3.9\n",
    "  before_script:\n",
    "    - pip install ghops\n",
    "  script:\n",
    "    - ghops list ${CI_PROJECT_DIR} > repos.json\n",
    "    - ghops status --format json > status.json\n",
    "    - ghops cluster analyze ${CI_PROJECT_DIR} > clusters.json\n",
    "  artifacts:\n",
    "    paths:\n",
    "      - repos.json\n",
    "      - status.json\n",
    "      - clusters.json\n",
    "    expire_in: 1 week\n",
    "\n",
    "consolidation:\n",
    "  stage: optimization\n",
    "  dependencies:\n",
    "    - repository-analysis\n",
    "  script:\n",
    "    - ghops workflow run consolidation-workflow --input clusters.json\n",
    "  only:\n",
    "    - schedules\n",
    "\"\"\"\n",
    "\n",
    "# Jenkins Pipeline\n",
    "jenkins_pipeline = \"\"\"\n",
    "pipeline {\n",
    "    agent any\n",
    "    \n",
    "    environment {\n",
    "        GHOPS_HOME = \"${WORKSPACE}/.ghops\"\n",
    "    }\n",
    "    \n",
    "    stages {\n",
    "        stage('Setup') {\n",
    "            steps {\n",
    "                sh 'pip install ghops'\n",
    "                sh 'ghops config init'\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        stage('Analysis') {\n",
    "            parallel {\n",
    "                stage('Clustering') {\n",
    "                    steps {\n",
    "                        sh 'ghops cluster analyze ${WORKSPACE}'\n",
    "                    }\n",
    "                }\n",
    "                stage('Status Check') {\n",
    "                    steps {\n",
    "                        sh 'ghops status ${WORKSPACE}'\n",
    "                    }\n",
    "                }\n",
    "                stage('Dependencies') {\n",
    "                    steps {\n",
    "                        sh 'ghops audit dependencies ${WORKSPACE}'\n",
    "                    }\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        stage('Workflow') {\n",
    "            steps {\n",
    "                sh 'ghops workflow run maintenance-workflow'\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    post {\n",
    "        always {\n",
    "            archiveArtifacts artifacts: '*.json,*.md'\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\"\"\"\n",
    "\n",
    "# Display CI/CD configurations\n",
    "print(\"CI/CD Integration Examples:\")\n",
    "print(\"=\" * 60)\n",
    "print(\"\\n1. GitHub Actions:\")\n",
    "print(github_action_workflow[:500] + \"...\")\n",
    "print(\"\\n2. GitLab CI:\")\n",
    "print(gitlab_ci_config[:400] + \"...\")\n",
    "print(\"\\n3. Jenkins Pipeline:\")\n",
    "print(jenkins_pipeline[:400] + \"...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Plugin Development {#plugins}\n",
    "\n",
    "Create plugins to extend ghops functionality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ghops Plugin Framework\n",
    "from abc import ABC, abstractmethod\n",
    "import importlib.util\n",
    "import inspect\n",
    "\n",
    "class GhopsPlugin(ABC):\n",
    "    \"\"\"Base class for ghops plugins\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.name = self.__class__.__name__\n",
    "        self.version = getattr(self, '__version__', '1.0.0')\n",
    "        self.description = self.__doc__ or \"No description\"\n",
    "        self.commands = {}\n",
    "        self.hooks = {}\n",
    "        self._register_commands()\n",
    "        self._register_hooks()\n",
    "    \n",
    "    @abstractmethod\n",
    "    def _register_commands(self):\n",
    "        \"\"\"Register plugin commands\"\"\"\n",
    "        pass\n",
    "    \n",
    "    def _register_hooks(self):\n",
    "        \"\"\"Register lifecycle hooks\"\"\"\n",
    "        pass\n",
    "    \n",
    "    def register_command(self, name: str, func, description: str = \"\"):\n",
    "        \"\"\"Register a new command\"\"\"\n",
    "        self.commands[name] = {\n",
    "            'function': func,\n",
    "            'description': description,\n",
    "            'params': inspect.signature(func).parameters\n",
    "        }\n",
    "    \n",
    "    def register_hook(self, event: str, func):\n",
    "        \"\"\"Register an event hook\"\"\"\n",
    "        if event not in self.hooks:\n",
    "            self.hooks[event] = []\n",
    "        self.hooks[event].append(func)\n",
    "\n",
    "# Example Plugin: Security Scanner\n",
    "class SecurityScannerPlugin(GhopsPlugin):\n",
    "    \"\"\"Advanced security scanning for repositories\"\"\"\n",
    "    \n",
    "    __version__ = '2.0.0'\n",
    "    \n",
    "    def _register_commands(self):\n",
    "        self.register_command('scan', self.scan_repository, \n",
    "                            'Scan repository for security issues')\n",
    "        self.register_command('audit', self.audit_dependencies,\n",
    "                            'Audit dependencies for vulnerabilities')\n",
    "        self.register_command('secrets', self.check_secrets,\n",
    "                            'Check for exposed secrets')\n",
    "    \n",
    "    def _register_hooks(self):\n",
    "        self.register_hook('pre_commit', self.pre_commit_scan)\n",
    "        self.register_hook('post_clone', self.post_clone_scan)\n",
    "    \n",
    "    def scan_repository(self, path: str, deep: bool = False):\n",
    "        \"\"\"Comprehensive security scan\"\"\"\n",
    "        results = {\n",
    "            'path': path,\n",
    "            'scan_date': datetime.now().isoformat(),\n",
    "            'issues': [],\n",
    "            'score': 100\n",
    "        }\n",
    "        \n",
    "        # Simulate scanning\n",
    "        checks = [\n",
    "            ('Hardcoded credentials', 'high', -30),\n",
    "            ('Outdated dependencies', 'medium', -15),\n",
    "            ('Missing security headers', 'low', -5),\n",
    "            ('Unencrypted sensitive data', 'critical', -50)\n",
    "        ]\n",
    "        \n",
    "        for check, severity, impact in checks:\n",
    "            if np.random.random() > 0.7:  # 30% chance of finding issue\n",
    "                results['issues'].append({\n",
    "                    'type': check,\n",
    "                    'severity': severity,\n",
    "                    'impact': impact,\n",
    "                    'file': f\"src/{np.random.choice(['main.py', 'config.yaml', '.env'])}\"\n",
    "                })\n",
    "                results['score'] += impact\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    def audit_dependencies(self, path: str):\n",
    "        \"\"\"Audit project dependencies\"\"\"\n",
    "        return {\n",
    "            'vulnerable_packages': np.random.randint(0, 5),\n",
    "            'outdated_packages': np.random.randint(0, 10),\n",
    "            'total_packages': np.random.randint(20, 50)\n",
    "        }\n",
    "    \n",
    "    def check_secrets(self, path: str):\n",
    "        \"\"\"Check for exposed secrets\"\"\"\n",
    "        patterns = ['API_KEY', 'SECRET_KEY', 'PASSWORD', 'TOKEN']\n",
    "        found = [p for p in patterns if np.random.random() > 0.8]\n",
    "        return {'exposed_secrets': found}\n",
    "    \n",
    "    def pre_commit_scan(self, files: List[str]):\n",
    "        \"\"\"Scan files before commit\"\"\"\n",
    "        print(f\"Scanning {len(files)} files before commit...\")\n",
    "        return all(self._quick_scan(f) for f in files)\n",
    "    \n",
    "    def post_clone_scan(self, repo_path: str):\n",
    "        \"\"\"Scan repository after cloning\"\"\"\n",
    "        print(f\"Security scanning cloned repository: {repo_path}\")\n",
    "        return self.scan_repository(repo_path)\n",
    "    \n",
    "    def _quick_scan(self, file: str):\n",
    "        \"\"\"Quick security scan for a single file\"\"\"\n",
    "        return np.random.random() > 0.1  # 90% pass rate\n",
    "\n",
    "# Example Plugin: Metrics Collector\n",
    "class MetricsCollectorPlugin(GhopsPlugin):\n",
    "    \"\"\"Collect and analyze repository metrics\"\"\"\n",
    "    \n",
    "    def _register_commands(self):\n",
    "        self.register_command('collect', self.collect_metrics,\n",
    "                            'Collect repository metrics')\n",
    "        self.register_command('analyze', self.analyze_trends,\n",
    "                            'Analyze metric trends')\n",
    "    \n",
    "    def collect_metrics(self, path: str):\n",
    "        \"\"\"Collect various repository metrics\"\"\"\n",
    "        return {\n",
    "            'lines_of_code': np.random.randint(1000, 10000),\n",
    "            'commit_count': np.random.randint(10, 500),\n",
    "            'contributor_count': np.random.randint(1, 20),\n",
    "            'open_issues': np.random.randint(0, 50),\n",
    "            'code_complexity': np.random.uniform(1, 10),\n",
    "            'test_coverage': np.random.uniform(30, 95)\n",
    "        }\n",
    "    \n",
    "    def analyze_trends(self, metrics_history: List[Dict]):\n",
    "        \"\"\"Analyze trends in metrics over time\"\"\"\n",
    "        if not metrics_history:\n",
    "            return {\"error\": \"No historical data\"}\n",
    "        \n",
    "        # Simulate trend analysis\n",
    "        trends = {\n",
    "            'code_growth': 'increasing',\n",
    "            'complexity_trend': 'stable',\n",
    "            'coverage_trend': 'improving',\n",
    "            'activity_level': 'high'\n",
    "        }\n",
    "        return trends\n",
    "\n",
    "# Plugin Manager\n",
    "class PluginManager:\n",
    "    \"\"\"Manage ghops plugins\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.plugins = {}\n",
    "        self.enabled_plugins = set()\n",
    "    \n",
    "    def register(self, plugin: GhopsPlugin):\n",
    "        \"\"\"Register a new plugin\"\"\"\n",
    "        self.plugins[plugin.name] = plugin\n",
    "        print(f\"Registered plugin: {plugin.name} v{plugin.version}\")\n",
    "    \n",
    "    def enable(self, plugin_name: str):\n",
    "        \"\"\"Enable a plugin\"\"\"\n",
    "        if plugin_name in self.plugins:\n",
    "            self.enabled_plugins.add(plugin_name)\n",
    "            print(f\"Enabled plugin: {plugin_name}\")\n",
    "        else:\n",
    "            print(f\"Plugin not found: {plugin_name}\")\n",
    "    \n",
    "    def execute_command(self, plugin_name: str, command: str, **kwargs):\n",
    "        \"\"\"Execute a plugin command\"\"\"\n",
    "        if plugin_name not in self.enabled_plugins:\n",
    "            return {\"error\": f\"Plugin {plugin_name} is not enabled\"}\n",
    "        \n",
    "        plugin = self.plugins[plugin_name]\n",
    "        if command not in plugin.commands:\n",
    "            return {\"error\": f\"Command {command} not found in plugin {plugin_name}\"}\n",
    "        \n",
    "        func = plugin.commands[command]['function']\n",
    "        return func(**kwargs)\n",
    "    \n",
    "    def list_plugins(self):\n",
    "        \"\"\"List all registered plugins\"\"\"\n",
    "        return [{\n",
    "            'name': p.name,\n",
    "            'version': p.version,\n",
    "            'description': p.description,\n",
    "            'enabled': p.name in self.enabled_plugins,\n",
    "            'commands': list(p.commands.keys())\n",
    "        } for p in self.plugins.values()]\n",
    "\n",
    "# Initialize and test plugins\n",
    "manager = PluginManager()\n",
    "\n",
    "# Register plugins\n",
    "security_plugin = SecurityScannerPlugin()\n",
    "metrics_plugin = MetricsCollectorPlugin()\n",
    "\n",
    "manager.register(security_plugin)\n",
    "manager.register(metrics_plugin)\n",
    "\n",
    "# Enable plugins\n",
    "manager.enable('SecurityScannerPlugin')\n",
    "manager.enable('MetricsCollectorPlugin')\n",
    "\n",
    "# List plugins\n",
    "print(\"\\nRegistered Plugins:\")\n",
    "print(\"=\" * 60)\n",
    "for plugin in manager.list_plugins():\n",
    "    status = \"âœ“\" if plugin['enabled'] else \"âœ—\"\n",
    "    print(f\"{status} {plugin['name']} v{plugin['version']}\")\n",
    "    print(f\"  {plugin['description']}\")\n",
    "    print(f\"  Commands: {', '.join(plugin['commands'])}\")\n",
    "\n",
    "# Test plugin execution\n",
    "print(\"\\nPlugin Execution Test:\")\n",
    "print(\"=\" * 60)\n",
    "result = manager.execute_command('SecurityScannerPlugin', 'scan', path='/test/repo')\n",
    "print(f\"Security Score: {result.get('score', 'N/A')}/100\")\n",
    "if result.get('issues'):\n",
    "    print(f\"Issues Found: {len(result['issues'])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. API Integration Patterns {#api-patterns}\n",
    "\n",
    "Learn patterns for integrating with external APIs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# API Integration Framework\n",
    "import asyncio\n",
    "from typing import Optional, Dict, Any, List\n",
    "from enum import Enum\n",
    "import hashlib\n",
    "import time\n",
    "\n",
    "class RateLimiter:\n",
    "    \"\"\"Rate limiter for API calls\"\"\"\n",
    "    \n",
    "    def __init__(self, calls_per_second: float = 1.0):\n",
    "        self.calls_per_second = calls_per_second\n",
    "        self.min_interval = 1.0 / calls_per_second\n",
    "        self.last_call = 0\n",
    "    \n",
    "    async def acquire(self):\n",
    "        \"\"\"Wait if necessary to respect rate limit\"\"\"\n",
    "        current = time.time()\n",
    "        time_since_last = current - self.last_call\n",
    "        \n",
    "        if time_since_last < self.min_interval:\n",
    "            await asyncio.sleep(self.min_interval - time_since_last)\n",
    "        \n",
    "        self.last_call = time.time()\n",
    "\n",
    "class CacheStrategy(Enum):\n",
    "    NONE = \"none\"\n",
    "    MEMORY = \"memory\"\n",
    "    DISK = \"disk\"\n",
    "    REDIS = \"redis\"\n",
    "\n",
    "class APIClient:\n",
    "    \"\"\"Base API client with common patterns\"\"\"\n",
    "    \n",
    "    def __init__(self, base_url: str, api_key: Optional[str] = None,\n",
    "                 rate_limit: float = 10.0, cache_strategy: CacheStrategy = CacheStrategy.MEMORY):\n",
    "        self.base_url = base_url\n",
    "        self.api_key = api_key\n",
    "        self.rate_limiter = RateLimiter(rate_limit)\n",
    "        self.cache_strategy = cache_strategy\n",
    "        self.cache = {} if cache_strategy == CacheStrategy.MEMORY else None\n",
    "        self.stats = {\n",
    "            'requests': 0,\n",
    "            'cache_hits': 0,\n",
    "            'errors': 0\n",
    "        }\n",
    "    \n",
    "    def _cache_key(self, endpoint: str, params: Dict) -> str:\n",
    "        \"\"\"Generate cache key for request\"\"\"\n",
    "        key_str = f\"{endpoint}:{json.dumps(params, sort_keys=True)}\"\n",
    "        return hashlib.md5(key_str.encode()).hexdigest()\n",
    "    \n",
    "    async def get(self, endpoint: str, params: Optional[Dict] = None) -> Dict[str, Any]:\n",
    "        \"\"\"Make GET request with caching and rate limiting\"\"\"\n",
    "        params = params or {}\n",
    "        \n",
    "        # Check cache\n",
    "        if self.cache_strategy == CacheStrategy.MEMORY:\n",
    "            cache_key = self._cache_key(endpoint, params)\n",
    "            if cache_key in self.cache:\n",
    "                self.stats['cache_hits'] += 1\n",
    "                return self.cache[cache_key]\n",
    "        \n",
    "        # Rate limiting\n",
    "        await self.rate_limiter.acquire()\n",
    "        \n",
    "        # Simulate API call\n",
    "        self.stats['requests'] += 1\n",
    "        \n",
    "        # Mock response\n",
    "        response = {\n",
    "            'endpoint': endpoint,\n",
    "            'params': params,\n",
    "            'data': self._generate_mock_data(endpoint),\n",
    "            'timestamp': datetime.now().isoformat()\n",
    "        }\n",
    "        \n",
    "        # Cache response\n",
    "        if self.cache_strategy == CacheStrategy.MEMORY:\n",
    "            self.cache[cache_key] = response\n",
    "        \n",
    "        return response\n",
    "    \n",
    "    def _generate_mock_data(self, endpoint: str) -> Any:\n",
    "        \"\"\"Generate mock data based on endpoint\"\"\"\n",
    "        if 'repos' in endpoint:\n",
    "            return [{'id': i, 'name': f'repo_{i}'} for i in range(5)]\n",
    "        elif 'user' in endpoint:\n",
    "            return {'id': 1, 'username': 'testuser', 'repos': 42}\n",
    "        else:\n",
    "            return {'status': 'ok'}\n",
    "    \n",
    "    def get_stats(self) -> Dict[str, int]:\n",
    "        \"\"\"Get client statistics\"\"\"\n",
    "        return self.stats.copy()\n",
    "\n",
    "# Specialized API Clients\n",
    "class GitHubAPIClient(APIClient):\n",
    "    \"\"\"GitHub API client with specific methods\"\"\"\n",
    "    \n",
    "    def __init__(self, token: str):\n",
    "        super().__init__(\n",
    "            base_url=\"https://api.github.com\",\n",
    "            api_key=token,\n",
    "            rate_limit=5000/3600  # 5000 requests per hour\n",
    "        )\n",
    "    \n",
    "    async def get_user_repos(self, username: str) -> List[Dict]:\n",
    "        \"\"\"Get user repositories\"\"\"\n",
    "        response = await self.get(f\"/users/{username}/repos\")\n",
    "        return response['data']\n",
    "    \n",
    "    async def get_repo_stats(self, owner: str, repo: str) -> Dict:\n",
    "        \"\"\"Get repository statistics\"\"\"\n",
    "        endpoints = [\n",
    "            f\"/repos/{owner}/{repo}\",\n",
    "            f\"/repos/{owner}/{repo}/stats/contributors\",\n",
    "            f\"/repos/{owner}/{repo}/languages\"\n",
    "        ]\n",
    "        \n",
    "        results = await asyncio.gather(*[self.get(ep) for ep in endpoints])\n",
    "        \n",
    "        return {\n",
    "            'info': results[0]['data'],\n",
    "            'contributors': results[1]['data'],\n",
    "            'languages': results[2]['data']\n",
    "        }\n",
    "\n",
    "class PyPIAPIClient(APIClient):\n",
    "    \"\"\"PyPI API client for package information\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        super().__init__(\n",
    "            base_url=\"https://pypi.org/pypi\",\n",
    "            rate_limit=10.0  # Conservative rate limit\n",
    "        )\n",
    "    \n",
    "    async def get_package_info(self, package_name: str) -> Dict:\n",
    "        \"\"\"Get package information from PyPI\"\"\"\n",
    "        response = await self.get(f\"/{package_name}/json\")\n",
    "        return response['data']\n",
    "    \n",
    "    async def check_updates(self, packages: List[tuple]) -> List[Dict]:\n",
    "        \"\"\"Check for package updates\"\"\"\n",
    "        updates = []\n",
    "        \n",
    "        for package, current_version in packages:\n",
    "            info = await self.get_package_info(package)\n",
    "            latest = info.get('version', '0.0.0')  # Mock version\n",
    "            \n",
    "            if latest != current_version:\n",
    "                updates.append({\n",
    "                    'package': package,\n",
    "                    'current': current_version,\n",
    "                    'latest': latest,\n",
    "                    'update_available': True\n",
    "                })\n",
    "        \n",
    "        return updates\n",
    "\n",
    "# Test API clients\n",
    "async def test_api_clients():\n",
    "    print(\"Testing API Clients:\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # GitHub client\n",
    "    github = GitHubAPIClient(\"mock_token\")\n",
    "    repos = await github.get_user_repos(\"octocat\")\n",
    "    print(f\"\\nGitHub: Found {len(repos)} repositories\")\n",
    "    \n",
    "    # PyPI client\n",
    "    pypi = PyPIAPIClient()\n",
    "    packages = [(\"requests\", \"2.25.0\"), (\"numpy\", \"1.19.0\")]\n",
    "    updates = await pypi.check_updates(packages)\n",
    "    print(f\"\\nPyPI: {len(updates)} packages have updates available\")\n",
    "    \n",
    "    # Show statistics\n",
    "    print(\"\\nAPI Client Statistics:\")\n",
    "    print(f\"GitHub: {github.get_stats()}\")\n",
    "    print(f\"PyPI: {pypi.get_stats()}\")\n",
    "\n",
    "# Run async test\n",
    "import nest_asyncio\n",
    "nest_asyncio.apply()\n",
    "loop = asyncio.get_event_loop()\n",
    "loop.run_until_complete(test_api_clients())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Event-Driven Automation {#event-driven}\n",
    "\n",
    "Build event-driven automation systems with ghops."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Event-Driven Architecture\n",
    "from typing import Callable, List, Dict, Any\n",
    "from dataclasses import dataclass, field\n",
    "from datetime import datetime\n",
    "import asyncio\n",
    "from enum import Enum\n",
    "\n",
    "class EventType(Enum):\n",
    "    REPO_CREATED = \"repo.created\"\n",
    "    REPO_UPDATED = \"repo.updated\"\n",
    "    REPO_DELETED = \"repo.deleted\"\n",
    "    COMMIT_PUSHED = \"commit.pushed\"\n",
    "    PR_OPENED = \"pr.opened\"\n",
    "    PR_MERGED = \"pr.merged\"\n",
    "    ISSUE_CREATED = \"issue.created\"\n",
    "    WORKFLOW_STARTED = \"workflow.started\"\n",
    "    WORKFLOW_COMPLETED = \"workflow.completed\"\n",
    "    CLUSTER_DETECTED = \"cluster.detected\"\n",
    "    SECURITY_ALERT = \"security.alert\"\n",
    "\n",
    "@dataclass\n",
    "class Event:\n",
    "    \"\"\"Represents an event in the system\"\"\"\n",
    "    type: EventType\n",
    "    timestamp: datetime = field(default_factory=datetime.now)\n",
    "    source: str = \"ghops\"\n",
    "    data: Dict[str, Any] = field(default_factory=dict)\n",
    "    metadata: Dict[str, Any] = field(default_factory=dict)\n",
    "\n",
    "class EventBus:\n",
    "    \"\"\"Central event bus for publishing and subscribing to events\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.subscribers: Dict[EventType, List[Callable]] = {}\n",
    "        self.event_history: List[Event] = []\n",
    "        self.filters: List[Callable] = []\n",
    "    \n",
    "    def subscribe(self, event_type: EventType, handler: Callable):\n",
    "        \"\"\"Subscribe to an event type\"\"\"\n",
    "        if event_type not in self.subscribers:\n",
    "            self.subscribers[event_type] = []\n",
    "        self.subscribers[event_type].append(handler)\n",
    "        print(f\"Subscribed {handler.__name__} to {event_type.value}\")\n",
    "    \n",
    "    def add_filter(self, filter_func: Callable[[Event], bool]):\n",
    "        \"\"\"Add a global event filter\"\"\"\n",
    "        self.filters.append(filter_func)\n",
    "    \n",
    "    async def publish(self, event: Event):\n",
    "        \"\"\"Publish an event to all subscribers\"\"\"\n",
    "        # Apply filters\n",
    "        for filter_func in self.filters:\n",
    "            if not filter_func(event):\n",
    "                return  # Event filtered out\n",
    "        \n",
    "        # Store in history\n",
    "        self.event_history.append(event)\n",
    "        \n",
    "        # Notify subscribers\n",
    "        if event.type in self.subscribers:\n",
    "            handlers = self.subscribers[event.type]\n",
    "            \n",
    "            # Run handlers concurrently\n",
    "            tasks = [self._run_handler(handler, event) for handler in handlers]\n",
    "            await asyncio.gather(*tasks)\n",
    "    \n",
    "    async def _run_handler(self, handler: Callable, event: Event):\n",
    "        \"\"\"Run an event handler safely\"\"\"\n",
    "        try:\n",
    "            if asyncio.iscoroutinefunction(handler):\n",
    "                await handler(event)\n",
    "            else:\n",
    "                handler(event)\n",
    "        except Exception as e:\n",
    "            print(f\"Error in handler {handler.__name__}: {e}\")\n",
    "    \n",
    "    def get_history(self, event_type: Optional[EventType] = None,\n",
    "                   limit: int = 100) -> List[Event]:\n",
    "        \"\"\"Get event history\"\"\"\n",
    "        history = self.event_history\n",
    "        if event_type:\n",
    "            history = [e for e in history if e.type == event_type]\n",
    "        return history[-limit:]\n",
    "\n",
    "# Event Handlers\n",
    "class AutomationHandlers:\n",
    "    \"\"\"Collection of automation event handlers\"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    async def on_repo_created(event: Event):\n",
    "        \"\"\"Handle repository creation\"\"\"\n",
    "        repo = event.data.get('repository', {})\n",
    "        print(f\"  â†’ Setting up new repository: {repo.get('name')}\")\n",
    "        \n",
    "        # Automated setup tasks\n",
    "        tasks = [\n",
    "            \"Initialize CI/CD pipeline\",\n",
    "            \"Add standard .gitignore\",\n",
    "            \"Create README template\",\n",
    "            \"Set up branch protection\"\n",
    "        ]\n",
    "        \n",
    "        for task in tasks:\n",
    "            await asyncio.sleep(0.1)  # Simulate work\n",
    "            print(f\"    âœ“ {task}\")\n",
    "    \n",
    "    @staticmethod\n",
    "    def on_security_alert(event: Event):\n",
    "        \"\"\"Handle security alerts\"\"\"\n",
    "        alert = event.data\n",
    "        severity = alert.get('severity', 'unknown')\n",
    "        \n",
    "        print(f\"  âš ï¸ SECURITY ALERT: {severity.upper()}\")\n",
    "        print(f\"    Repository: {alert.get('repository')}\")\n",
    "        print(f\"    Issue: {alert.get('issue')}\")\n",
    "        \n",
    "        if severity == 'critical':\n",
    "            print(\"    ðŸš¨ Initiating emergency response protocol\")\n",
    "    \n",
    "    @staticmethod\n",
    "    async def on_cluster_detected(event: Event):\n",
    "        \"\"\"Handle cluster detection\"\"\"\n",
    "        cluster = event.data\n",
    "        similarity = cluster.get('similarity', 0)\n",
    "        \n",
    "        if similarity > 0.9:\n",
    "            print(f\"  ðŸ” High similarity cluster detected: {similarity:.1%}\")\n",
    "            print(f\"    Members: {cluster.get('members', [])}\")\n",
    "            print(\"    â†’ Triggering consolidation workflow\")\n",
    "    \n",
    "    @staticmethod\n",
    "    def on_workflow_completed(event: Event):\n",
    "        \"\"\"Handle workflow completion\"\"\"\n",
    "        workflow = event.data\n",
    "        status = workflow.get('status')\n",
    "        duration = workflow.get('duration', 0)\n",
    "        \n",
    "        icon = \"âœ…\" if status == 'success' else \"âŒ\"\n",
    "        print(f\"  {icon} Workflow '{workflow.get('name')}' completed\")\n",
    "        print(f\"    Duration: {duration:.1f}s\")\n",
    "        print(f\"    Status: {status}\")\n",
    "\n",
    "# Create and configure event bus\n",
    "event_bus = EventBus()\n",
    "handlers = AutomationHandlers()\n",
    "\n",
    "# Subscribe handlers to events\n",
    "event_bus.subscribe(EventType.REPO_CREATED, handlers.on_repo_created)\n",
    "event_bus.subscribe(EventType.SECURITY_ALERT, handlers.on_security_alert)\n",
    "event_bus.subscribe(EventType.CLUSTER_DETECTED, handlers.on_cluster_detected)\n",
    "event_bus.subscribe(EventType.WORKFLOW_COMPLETED, handlers.on_workflow_completed)\n",
    "\n",
    "# Add event filter (only process events from last hour)\n",
    "def recent_events_filter(event: Event) -> bool:\n",
    "    age = datetime.now() - event.timestamp\n",
    "    return age.total_seconds() < 3600  # 1 hour\n",
    "\n",
    "event_bus.add_filter(recent_events_filter)\n",
    "\n",
    "# Simulate events\n",
    "async def simulate_events():\n",
    "    print(\"\\nSimulating Events:\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    events = [\n",
    "        Event(\n",
    "            type=EventType.REPO_CREATED,\n",
    "            data={'repository': {'name': 'new-project', 'language': 'python'}}\n",
    "        ),\n",
    "        Event(\n",
    "            type=EventType.SECURITY_ALERT,\n",
    "            data={'severity': 'critical', 'repository': 'api-server', \n",
    "                  'issue': 'SQL injection vulnerability detected'}\n",
    "        ),\n",
    "        Event(\n",
    "            type=EventType.CLUSTER_DETECTED,\n",
    "            data={'similarity': 0.92, 'members': ['service-a', 'service-b', 'service-c']}\n",
    "        ),\n",
    "        Event(\n",
    "            type=EventType.WORKFLOW_COMPLETED,\n",
    "            data={'name': 'deployment-pipeline', 'status': 'success', 'duration': 142.3}\n",
    "        )\n",
    "    ]\n",
    "    \n",
    "    for event in events:\n",
    "        print(f\"\\nðŸ“¡ Event: {event.type.value}\")\n",
    "        await event_bus.publish(event)\n",
    "        await asyncio.sleep(0.5)\n",
    "\n",
    "# Run simulation\n",
    "loop = asyncio.get_event_loop()\n",
    "loop.run_until_complete(simulate_events())\n",
    "\n",
    "# Show event history\n",
    "print(\"\\nEvent History:\")\n",
    "print(\"=\" * 60)\n",
    "for event in event_bus.get_history(limit=5):\n",
    "    print(f\"{event.timestamp.strftime('%H:%M:%S')} - {event.type.value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Cross-Platform Orchestration {#cross-platform}\n",
    "\n",
    "Orchestrate operations across multiple platforms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cross-Platform Orchestration System\n",
    "@dataclass\n",
    "class Platform:\n",
    "    name: str\n",
    "    type: str\n",
    "    api_endpoint: str\n",
    "    capabilities: List[str] = field(default_factory=list)\n",
    "    credentials: Optional[Dict] = None\n",
    "\n",
    "class PlatformOrchestrator:\n",
    "    \"\"\"Orchestrate operations across multiple platforms\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.platforms = {}\n",
    "        self.workflows = {}\n",
    "        self._register_platforms()\n",
    "    \n",
    "    def _register_platforms(self):\n",
    "        \"\"\"Register supported platforms\"\"\"\n",
    "        platforms = [\n",
    "            Platform(\"GitHub\", \"vcs\", \"https://api.github.com\",\n",
    "                    [\"repos\", \"issues\", \"actions\", \"releases\"]),\n",
    "            Platform(\"GitLab\", \"vcs\", \"https://gitlab.com/api/v4\",\n",
    "                    [\"repos\", \"ci\", \"registry\"]),\n",
    "            Platform(\"PyPI\", \"registry\", \"https://pypi.org\",\n",
    "                    [\"publish\", \"search\", \"stats\"]),\n",
    "            Platform(\"DockerHub\", \"registry\", \"https://hub.docker.com\",\n",
    "                    [\"images\", \"push\", \"scan\"]),\n",
    "            Platform(\"AWS\", \"cloud\", \"https://aws.amazon.com\",\n",
    "                    [\"deploy\", \"lambda\", \"s3\"]),\n",
    "            Platform(\"Kubernetes\", \"orchestration\", \"https://k8s.io\",\n",
    "                    [\"deploy\", \"scale\", \"monitor\"])\n",
    "        ]\n",
    "        \n",
    "        for platform in platforms:\n",
    "            self.platforms[platform.name] = platform\n",
    "    \n",
    "    async def orchestrate_release(self, project: str, version: str):\n",
    "        \"\"\"Orchestrate a release across multiple platforms\"\"\"\n",
    "        print(f\"\\nðŸŽ­ Orchestrating release for {project} v{version}\")\n",
    "        print(\"=\" * 60)\n",
    "        \n",
    "        steps = [\n",
    "            (\"GitHub\", \"Create release tag\", self._github_release),\n",
    "            (\"GitLab\", \"Mirror to GitLab\", self._gitlab_mirror),\n",
    "            (\"PyPI\", \"Publish package\", self._pypi_publish),\n",
    "            (\"DockerHub\", \"Build & push image\", self._docker_push),\n",
    "            (\"AWS\", \"Deploy to Lambda\", self._aws_deploy),\n",
    "            (\"Kubernetes\", \"Update deployment\", self._k8s_update)\n",
    "        ]\n",
    "        \n",
    "        results = []\n",
    "        for platform, action, func in steps:\n",
    "            print(f\"\\nðŸ“ {platform}: {action}\")\n",
    "            try:\n",
    "                result = await func(project, version)\n",
    "                results.append((platform, \"success\", result))\n",
    "                print(f\"  âœ“ Success: {result}\")\n",
    "            except Exception as e:\n",
    "                results.append((platform, \"failed\", str(e)))\n",
    "                print(f\"  âœ— Failed: {e}\")\n",
    "            \n",
    "            await asyncio.sleep(0.5)  # Simulate processing time\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    async def _github_release(self, project: str, version: str):\n",
    "        \"\"\"Create GitHub release\"\"\"\n",
    "        return f\"Created release v{version} with 3 assets\"\n",
    "    \n",
    "    async def _gitlab_mirror(self, project: str, version: str):\n",
    "        \"\"\"Mirror to GitLab\"\"\"\n",
    "        return f\"Mirrored {project} to GitLab\"\n",
    "    \n",
    "    async def _pypi_publish(self, project: str, version: str):\n",
    "        \"\"\"Publish to PyPI\"\"\"\n",
    "        return f\"Published {project}-{version}.tar.gz\"\n",
    "    \n",
    "    async def _docker_push(self, project: str, version: str):\n",
    "        \"\"\"Push Docker image\"\"\"\n",
    "        return f\"Pushed {project}:{version} to registry\"\n",
    "    \n",
    "    async def _aws_deploy(self, project: str, version: str):\n",
    "        \"\"\"Deploy to AWS\"\"\"\n",
    "        if np.random.random() > 0.8:  # 20% chance of failure\n",
    "            raise Exception(\"Lambda function update failed\")\n",
    "        return f\"Deployed to Lambda function {project}-prod\"\n",
    "    \n",
    "    async def _k8s_update(self, project: str, version: str):\n",
    "        \"\"\"Update Kubernetes deployment\"\"\"\n",
    "        return f\"Updated deployment {project} to v{version}\"\n",
    "    \n",
    "    def get_platform_matrix(self) -> pd.DataFrame:\n",
    "        \"\"\"Get platform capability matrix\"\"\"\n",
    "        all_capabilities = set()\n",
    "        for platform in self.platforms.values():\n",
    "            all_capabilities.update(platform.capabilities)\n",
    "        \n",
    "        matrix_data = []\n",
    "        for platform in self.platforms.values():\n",
    "            row = {'Platform': platform.name, 'Type': platform.type}\n",
    "            for cap in all_capabilities:\n",
    "                row[cap] = 'âœ“' if cap in platform.capabilities else ''\n",
    "            matrix_data.append(row)\n",
    "        \n",
    "        return pd.DataFrame(matrix_data)\n",
    "\n",
    "# Initialize orchestrator\n",
    "orchestrator = PlatformOrchestrator()\n",
    "\n",
    "# Display platform matrix\n",
    "print(\"Platform Capability Matrix:\")\n",
    "print(\"=\" * 80)\n",
    "matrix = orchestrator.get_platform_matrix()\n",
    "print(matrix.to_string(index=False))\n",
    "\n",
    "# Simulate cross-platform release\n",
    "async def test_orchestration():\n",
    "    results = await orchestrator.orchestrate_release(\"awesome-project\", \"2.0.0\")\n",
    "    \n",
    "    print(\"\\n\\nðŸ“Š Orchestration Summary:\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    success_count = sum(1 for _, status, _ in results if status == \"success\")\n",
    "    total = len(results)\n",
    "    \n",
    "    print(f\"Success Rate: {success_count}/{total} ({success_count/total*100:.0f}%)\")\n",
    "    \n",
    "    for platform, status, detail in results:\n",
    "        icon = \"âœ“\" if status == \"success\" else \"âœ—\"\n",
    "        print(f\"{icon} {platform}: {detail}\")\n",
    "\n",
    "# Run orchestration test\n",
    "loop = asyncio.get_event_loop()\n",
    "loop.run_until_complete(test_orchestration())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Real-World Use Cases {#use-cases}\n",
    "\n",
    "Explore real-world integration scenarios."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Real-World Use Case: Multi-Repository Refactoring\n",
    "class MultiRepoRefactoring:\n",
    "    \"\"\"Coordinate refactoring across multiple repositories\"\"\"\n",
    "    \n",
    "    def __init__(self, repos: List[str]):\n",
    "        self.repos = repos\n",
    "        self.changes = {}\n",
    "        self.validation_results = {}\n",
    "    \n",
    "    async def plan_refactoring(self, old_pattern: str, new_pattern: str):\n",
    "        \"\"\"Plan refactoring changes\"\"\"\n",
    "        print(f\"Planning refactoring: '{old_pattern}' â†’ '{new_pattern}'\")\n",
    "        print(\"=\" * 60)\n",
    "        \n",
    "        for repo in self.repos:\n",
    "            # Simulate finding occurrences\n",
    "            occurrences = np.random.randint(0, 20)\n",
    "            files_affected = np.random.randint(0, 10)\n",
    "            \n",
    "            self.changes[repo] = {\n",
    "                'occurrences': occurrences,\n",
    "                'files': files_affected,\n",
    "                'estimated_time': occurrences * 0.5  # minutes\n",
    "            }\n",
    "            \n",
    "            print(f\"  {repo}: {occurrences} occurrences in {files_affected} files\")\n",
    "        \n",
    "        total_occurrences = sum(c['occurrences'] for c in self.changes.values())\n",
    "        total_time = sum(c['estimated_time'] for c in self.changes.values())\n",
    "        \n",
    "        print(f\"\\nTotal: {total_occurrences} changes, estimated {total_time:.1f} minutes\")\n",
    "        return self.changes\n",
    "    \n",
    "    async def validate_changes(self):\n",
    "        \"\"\"Validate proposed changes\"\"\"\n",
    "        print(\"\\nValidating changes...\")\n",
    "        \n",
    "        for repo in self.repos:\n",
    "            # Simulate validation\n",
    "            tests_pass = np.random.random() > 0.2  # 80% pass rate\n",
    "            build_success = np.random.random() > 0.1  # 90% success rate\n",
    "            \n",
    "            self.validation_results[repo] = {\n",
    "                'tests': 'pass' if tests_pass else 'fail',\n",
    "                'build': 'success' if build_success else 'failed',\n",
    "                'safe': tests_pass and build_success\n",
    "            }\n",
    "        \n",
    "        safe_repos = [r for r, v in self.validation_results.items() if v['safe']]\n",
    "        print(f\"  Safe to refactor: {len(safe_repos)}/{len(self.repos)} repositories\")\n",
    "        \n",
    "        return self.validation_results\n",
    "    \n",
    "    async def execute_refactoring(self):\n",
    "        \"\"\"Execute refactoring with rollback capability\"\"\"\n",
    "        print(\"\\nExecuting refactoring...\")\n",
    "        \n",
    "        completed = []\n",
    "        failed = []\n",
    "        \n",
    "        for repo in self.repos:\n",
    "            if self.validation_results.get(repo, {}).get('safe', False):\n",
    "                # Simulate refactoring\n",
    "                success = np.random.random() > 0.05  # 95% success rate\n",
    "                \n",
    "                if success:\n",
    "                    completed.append(repo)\n",
    "                    print(f\"  âœ“ {repo}: Refactoring complete\")\n",
    "                else:\n",
    "                    failed.append(repo)\n",
    "                    print(f\"  âœ— {repo}: Refactoring failed, rolling back\")\n",
    "            else:\n",
    "                print(f\"  âš  {repo}: Skipped (validation failed)\")\n",
    "        \n",
    "        return {'completed': completed, 'failed': failed}\n",
    "\n",
    "# Real-World Use Case: Dependency Update Campaign\n",
    "class DependencyUpdateCampaign:\n",
    "    \"\"\"Manage dependency updates across portfolio\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.vulnerabilities = {}\n",
    "        self.updates = {}\n",
    "        self.priorities = {}\n",
    "    \n",
    "    async def scan_vulnerabilities(self, repos: List[str]):\n",
    "        \"\"\"Scan for security vulnerabilities\"\"\"\n",
    "        print(\"Scanning for vulnerabilities...\")\n",
    "        \n",
    "        critical_packages = ['log4j', 'openssl', 'requests']\n",
    "        \n",
    "        for repo in repos:\n",
    "            vulns = []\n",
    "            for pkg in critical_packages:\n",
    "                if np.random.random() > 0.7:  # 30% chance of vulnerability\n",
    "                    vulns.append({\n",
    "                        'package': pkg,\n",
    "                        'severity': np.random.choice(['low', 'medium', 'high', 'critical']),\n",
    "                        'cve': f\"CVE-2024-{np.random.randint(1000, 9999)}\"\n",
    "                    })\n",
    "            \n",
    "            self.vulnerabilities[repo] = vulns\n",
    "        \n",
    "        total_vulns = sum(len(v) for v in self.vulnerabilities.values())\n",
    "        critical = sum(1 for r in self.vulnerabilities.values() \n",
    "                      for v in r if v['severity'] == 'critical')\n",
    "        \n",
    "        print(f\"  Found {total_vulns} vulnerabilities ({critical} critical)\")\n",
    "        return self.vulnerabilities\n",
    "    \n",
    "    def prioritize_updates(self):\n",
    "        \"\"\"Prioritize update order\"\"\"\n",
    "        for repo, vulns in self.vulnerabilities.items():\n",
    "            # Calculate priority score\n",
    "            score = 0\n",
    "            for vuln in vulns:\n",
    "                if vuln['severity'] == 'critical':\n",
    "                    score += 100\n",
    "                elif vuln['severity'] == 'high':\n",
    "                    score += 50\n",
    "                elif vuln['severity'] == 'medium':\n",
    "                    score += 20\n",
    "                else:\n",
    "                    score += 5\n",
    "            \n",
    "            self.priorities[repo] = score\n",
    "        \n",
    "        # Sort by priority\n",
    "        sorted_repos = sorted(self.priorities.items(), key=lambda x: x[1], reverse=True)\n",
    "        return sorted_repos\n",
    "\n",
    "# Test real-world scenarios\n",
    "print(\"Real-World Integration Scenarios\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Scenario 1: Multi-repo refactoring\n",
    "print(\"\\nðŸ“ Scenario 1: Multi-Repository Refactoring\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "repos = [f\"service-{chr(65+i)}\" for i in range(5)]\n",
    "refactoring = MultiRepoRefactoring(repos)\n",
    "\n",
    "async def run_refactoring():\n",
    "    await refactoring.plan_refactoring(\"OldAPIClient\", \"NewAPIClient\")\n",
    "    await refactoring.validate_changes()\n",
    "    result = await refactoring.execute_refactoring()\n",
    "    print(f\"\\nRefactoring complete: {len(result['completed'])} succeeded, {len(result['failed'])} failed\")\n",
    "\n",
    "loop = asyncio.get_event_loop()\n",
    "loop.run_until_complete(run_refactoring())\n",
    "\n",
    "# Scenario 2: Dependency updates\n",
    "print(\"\\nðŸ”’ Scenario 2: Security Dependency Update Campaign\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "campaign = DependencyUpdateCampaign()\n",
    "\n",
    "async def run_campaign():\n",
    "    await campaign.scan_vulnerabilities(repos)\n",
    "    priorities = campaign.prioritize_updates()\n",
    "    \n",
    "    print(\"\\nUpdate Priority Order:\")\n",
    "    for repo, score in priorities[:3]:\n",
    "        print(f\"  1. {repo} (priority score: {score})\")\n",
    "\n",
    "loop.run_until_complete(run_campaign())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Performance Optimization {#performance}\n",
    "\n",
    "Optimize integration performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Performance optimization strategies\n",
    "import time\n",
    "import concurrent.futures\n",
    "from functools import lru_cache\n",
    "\n",
    "class PerformanceOptimizer:\n",
    "    \"\"\"Optimize ghops integration performance\"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def measure_time(func):\n",
    "        \"\"\"Decorator to measure function execution time\"\"\"\n",
    "        def wrapper(*args, **kwargs):\n",
    "            start = time.time()\n",
    "            result = func(*args, **kwargs)\n",
    "            duration = time.time() - start\n",
    "            print(f\"  {func.__name__}: {duration:.3f}s\")\n",
    "            return result, duration\n",
    "        return wrapper\n",
    "    \n",
    "    @measure_time\n",
    "    def sequential_processing(self, items: List[Any], processor):\n",
    "        \"\"\"Process items sequentially\"\"\"\n",
    "        results = []\n",
    "        for item in items:\n",
    "            results.append(processor(item))\n",
    "        return results\n",
    "    \n",
    "    @measure_time\n",
    "    def parallel_processing(self, items: List[Any], processor, max_workers=4):\n",
    "        \"\"\"Process items in parallel\"\"\"\n",
    "        with concurrent.futures.ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "            results = list(executor.map(processor, items))\n",
    "        return results\n",
    "    \n",
    "    @measure_time\n",
    "    def batch_processing(self, items: List[Any], processor, batch_size=10):\n",
    "        \"\"\"Process items in batches\"\"\"\n",
    "        results = []\n",
    "        for i in range(0, len(items), batch_size):\n",
    "            batch = items[i:i+batch_size]\n",
    "            results.extend(processor(batch))\n",
    "        return results\n",
    "    \n",
    "    @lru_cache(maxsize=128)\n",
    "    def cached_expensive_operation(self, key: str):\n",
    "        \"\"\"Cache expensive operations\"\"\"\n",
    "        time.sleep(0.1)  # Simulate expensive operation\n",
    "        return f\"Result for {key}\"\n",
    "\n",
    "# Test performance optimizations\n",
    "print(\"Performance Optimization Comparison\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "optimizer = PerformanceOptimizer()\n",
    "\n",
    "# Simulate processing function\n",
    "def process_item(item):\n",
    "    time.sleep(0.01)  # Simulate work\n",
    "    return item * 2\n",
    "\n",
    "def process_batch(batch):\n",
    "    time.sleep(0.01 * len(batch))  # Simulate batch processing\n",
    "    return [item * 2 for item in batch]\n",
    "\n",
    "# Test data\n",
    "test_items = list(range(100))\n",
    "\n",
    "print(\"\\nProcessing 100 items:\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "# Sequential\n",
    "seq_results, seq_time = optimizer.sequential_processing(test_items, process_item)\n",
    "\n",
    "# Parallel\n",
    "par_results, par_time = optimizer.parallel_processing(test_items, process_item, max_workers=10)\n",
    "\n",
    "# Batch\n",
    "batch_results, batch_time = optimizer.batch_processing(test_items, process_batch, batch_size=20)\n",
    "\n",
    "# Calculate speedup\n",
    "print(\"\\nSpeedup Analysis:\")\n",
    "print(f\"  Parallel vs Sequential: {seq_time/par_time:.2f}x faster\")\n",
    "print(f\"  Batch vs Sequential: {seq_time/batch_time:.2f}x faster\")\n",
    "\n",
    "# Cache effectiveness\n",
    "print(\"\\nCache Effectiveness:\")\n",
    "cache_keys = ['key1', 'key2', 'key1', 'key3', 'key2', 'key1']  # Some repeated\n",
    "\n",
    "start = time.time()\n",
    "for key in cache_keys:\n",
    "    optimizer.cached_expensive_operation(key)\n",
    "cached_time = time.time() - start\n",
    "\n",
    "print(f\"  Processing {len(cache_keys)} items (with cache): {cached_time:.3f}s\")\n",
    "print(f\"  Cache info: {optimizer.cached_expensive_operation.cache_info()}\")\n",
    "\n",
    "# Optimization recommendations\n",
    "print(\"\\nðŸ’¡ Optimization Recommendations:\")\n",
    "print(\"=\" * 60)\n",
    "recommendations = [\n",
    "    \"Use parallel processing for I/O-bound operations\",\n",
    "    \"Implement caching for expensive API calls\",\n",
    "    \"Batch database operations to reduce overhead\",\n",
    "    \"Use connection pooling for network requests\",\n",
    "    \"Implement circuit breakers for external services\",\n",
    "    \"Use async/await for concurrent operations\",\n",
    "    \"Profile code to identify bottlenecks\"\n",
    "]\n",
    "\n",
    "for i, rec in enumerate(recommendations, 1):\n",
    "    print(f\"{i}. {rec}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Exercises {#exercises}\n",
    "\n",
    "Practice advanced integration techniques."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1: Custom Integration\n",
    "Create a custom integration between ghops and a third-party service."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Create a custom integration\n",
    "# Requirements:\n",
    "# 1. Define a new platform or service\n",
    "# 2. Implement authentication\n",
    "# 3. Create API client with rate limiting\n",
    "# 4. Build custom actions\n",
    "# 5. Handle errors gracefully\n",
    "\n",
    "class MyCustomIntegration:\n",
    "    \"\"\"Your custom integration here\"\"\"\n",
    "    pass\n",
    "\n",
    "# Test your integration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2: Event-Driven Workflow\n",
    "Design an event-driven workflow that responds to repository events."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Create an event-driven workflow\n",
    "# Requirements:\n",
    "# 1. Define custom events\n",
    "# 2. Create event handlers\n",
    "# 3. Implement event filtering\n",
    "# 4. Add error recovery\n",
    "# 5. Create event analytics\n",
    "\n",
    "# Your implementation here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3: Performance Tuning\n",
    "Optimize a slow integration workflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Optimize this slow workflow\n",
    "# Current workflow processes repositories one by one\n",
    "# Goal: Achieve 5x speedup\n",
    "\n",
    "def slow_workflow(repos: List[str]):\n",
    "    \"\"\"Slow workflow that needs optimization\"\"\"\n",
    "    results = []\n",
    "    for repo in repos:\n",
    "        # Slow operations\n",
    "        status = check_status(repo)  # 1s\n",
    "        metrics = collect_metrics(repo)  # 2s\n",
    "        analysis = analyze_code(repo)  # 3s\n",
    "        results.append({'status': status, 'metrics': metrics, 'analysis': analysis})\n",
    "    return results\n",
    "\n",
    "# Your optimized version\n",
    "def optimized_workflow(repos: List[str]):\n",
    "    \"\"\"Your optimized implementation\"\"\"\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean up workspace\n",
    "import shutil\n",
    "if 'workspace' in locals() and os.path.exists(workspace):\n",
    "    shutil.rmtree(workspace)\n",
    "    print(f\"Cleaned up workspace: {workspace}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "In this notebook, you learned:\n",
    "- Combining clustering with workflow automation\n",
    "- Building custom actions and plugins\n",
    "- Integrating with CI/CD systems\n",
    "- Creating event-driven automation\n",
    "- Cross-platform orchestration\n",
    "- API integration patterns\n",
    "- Performance optimization techniques\n",
    "- Real-world integration scenarios\n",
    "\n",
    "## Key Takeaways\n",
    "\n",
    "1. **Intelligent Automation**: Use clustering to drive smart workflow decisions\n",
    "2. **Extensibility**: Custom actions and plugins extend ghops capabilities\n",
    "3. **CI/CD Integration**: Seamlessly integrate with existing DevOps pipelines\n",
    "4. **Event-Driven**: React to changes in real-time with event-driven patterns\n",
    "5. **Cross-Platform**: Orchestrate operations across multiple services\n",
    "6. **Performance**: Optimization techniques can provide significant speedups\n",
    "7. **Real-World**: Apply patterns to solve actual development challenges\n",
    "\n",
    "## Next Steps\n",
    "\n",
    "- **Notebook 5**: Data Analysis and Visualization - Deep dive into repository analytics\n",
    "- Build your own custom integrations\n",
    "- Contribute to the ghops ecosystem\n",
    "- Share your workflows with the community"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}